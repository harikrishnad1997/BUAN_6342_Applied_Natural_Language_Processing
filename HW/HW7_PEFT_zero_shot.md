Certainly! Here's your homework tasks formatted in markdown:

---

# Homework Tasks:

## Task 1:
### Model: `google/gemma-1.1-2b-it` or a similar size latest model like phi-3
#### Part A: Fine-tune using LoRA.
#### Part B: Employ a parameter-efficient fine-tuning method other than LoRA.

## Task 2:
### Select a model from the MTEB benchmark ([MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)) and fine-tune it using QLoRA.

## Task 3:
### Optional Task: Zero-Shot Classification
#### Utilize any large model for zero-shot or few-shot classification.

## Final Report Requirements:
- A clear explanation of the tasks and datasets used.
- A results table with the models tested and their respective performances.
- A public link to your WandB project. Note: Inability to access the link will result in your assignment not being graded.
- A discussion on hyperparameter selection strategies employed for fine-tuning the models.
- A conclusion section outlining the challenges faced and learnings gained.

## Required Submissions for HW:
- Submit code files on eLearning.
- Submit report (word or pdf) on eLearning.
- You should have at least two submissions on Kaggle (Minimum Score of 0.45 on Kaggle).

## Initial Kaggle Competition Submission:
- Make sure to submit an initial entry to our in-class Kaggle competition based on the solutions from HW6 by May 1. Failure to submit by this deadline will result in disqualification from the competition and your HW7 will not be graded.

## Extra Grade:
- The top five students in Kaggle Competition across two sections will receive 3 bonus points.
