{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YlotA24NhbxB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, HalvingGridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "import warnings\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from CustomPreprocessorSpacy import SpacyPreprocessor\n",
        "# Filter out warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import warnings\n",
        "from sklearn.experimental import enable_halving_search_cv # noqa\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "import ast\n",
        "\n",
        "# Filter out warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Read the CSV files containing features and labels\n",
        "df_fe = pd.read_csv('spam_bert_tfe.csv')\n",
        "df_fe['message_features'] = df_fe['message_features'].apply(ast.literal_eval)\n",
        "df_fe = pd.DataFrame(df_fe['message_features'].apply(pd.Series))\n",
        "\n",
        "df = pd.read_csv(\"spam.csv\", encoding='latin1')\n",
        "\n",
        "# Keep only the necessary columns and rename them\n",
        "df = df[['v1', 'v2']]\n",
        "df.columns = ['label', 'message']\n",
        "\n",
        "# Convert labels to numerical values\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Preprocess text data using SpacyPreprocessor\n",
        "sp = SpacyPreprocessor('en_core_web_sm')\n",
        "df['message'] = sp.transform(df['message'].values)\n",
        "\n",
        "# Combine the features and labels\n",
        "df_fe['label'] = df['label']\n",
        "df_fe['message'] = df['message']\n",
        "\n",
        "# Convert column names to string\n",
        "df_fe.columns = df_fe.columns.astype(str)\n",
        "\n",
        "# Shuffle the data\n",
        "sampled_df = df_fe.sample(frac=1, random_state=42)\n",
        "\n",
        "# Define features (X) and labels (y)\n",
        "X = sampled_df.drop(columns=['label'])\n",
        "y = sampled_df['label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize classifiers\n",
        "log_reg_clf = LogisticRegression()\n",
        "svc_clf = SVC()\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "rf_clf = RandomForestClassifier()\n",
        "\n",
        "# Define parameter grids for grid search\n",
        "param_grids = [\n",
        "    {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},  # C values for Logistic Regression\n",
        "    {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100], 'classifier__gamma': [0.01, 0.1, 1, 10, 100]},  # C and gamma values for SVC\n",
        "    {'classifier__max_depth': [None, 5, 10, 15, 20, 25, 30]},  # max_depth values for Decision Tree\n",
        "    {'classifier__n_estimators': [10, 50, 100, 200, 500], 'classifier__max_features': ['auto', 'sqrt', 'log2']}  # n_estimators and max_features values for Random Forest\n",
        "]\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = [log_reg_clf, svc_clf, dt_clf, rf_clf]\n",
        "\n",
        "# Initialize variables to store best model information\n",
        "best_weighted_f1_score = 0\n",
        "best_model = None\n",
        "best_clf_name = None\n",
        "\n",
        "# Initialize TF-IDF Vectorizer and StandardScaler\n",
        "tfidf_transformer = TfidfVectorizer()\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Define ColumnTransformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('tfidf', tfidf_transformer, 'message'),  # TF-IDF Vectorization for text data\n",
        "        ('scaler', scaler, [col for col in X.columns if col != 'message'])  # Standard Scaling for numerical columns\n",
        "    ],\n",
        "    remainder='passthrough'  # Ensure other columns not mentioned are passed through\n",
        ")\n",
        "\n",
        "# Iterate over classifiers and corresponding parameter grids\n",
        "for classifier, param_grid in zip(classifiers, param_grids):\n",
        "    # Define pipeline\n",
        "    pipeline = Pipeline([\n",
        "      ('preprocessor', preprocessor),\n",
        "      ('classifier', classifier)\n",
        "    ])\n",
        "\n",
        "    # Perform grid search for hyperparameter tuning\n",
        "    grid_search = HalvingGridSearchCV(pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Predict labels on test data\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Print classifier information and evaluation metrics\n",
        "    print(\"Classifier:\", classifier.__class__.__name__)\n",
        "    print(\"Best Parameters:\", grid_search.best_params_)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"-------------------------------------------\\n\")\n",
        "\n",
        "    # Save the best model if it achieves a higher accuracy\n",
        "    if accuracy > best_weighted_f1_score:\n",
        "        best_weighted_f1_score = accuracy\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_clf_name = classifier.__class__.__name__\n",
        "\n",
        "# Save the best model\n",
        "if best_model is not None:\n",
        "    joblib.dump(best_model, f\"best_model_{best_clf_name}_big_data_fe_full.joblib\")\n",
        "    print(f\"Best model saved as best_model_{best_clf_name}_big_data_fe_full.joblib\")\n",
        "\n",
        "# Reload the best model\n",
        "best_model = joblib.load(f\"best_model_{best_clf_name}_big_data_fe_full.joblib\")\n",
        "\n",
        "# Make predictions on the test data using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on test data using the best model:\", accuracy)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot learning curves (uncomment if needed)\n",
        "# from plot_learning_curve import plot_learning_curve\n",
        "# plot_learning_curve(best_model, 'Learning Curves', X_train, y_train, n_jobs=-1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPoCRUMnhmix",
        "outputId": "fdc2eb38-41ef-42ab-cebc-04a04e8ebe7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 2\n",
            "n_required_iterations: 2\n",
            "n_possible_iterations: 2\n",
            "min_resources_: 1485\n",
            "max_resources_: 4457\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 7\n",
            "n_resources: 1485\n",
            "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 3\n",
            "n_resources: 4455\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Classifier: LogisticRegression\n",
            "Best Parameters: {'classifier__C': 0.1}\n",
            "Accuracy: 0.9874439461883409\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       949\n",
            "           1       0.97      0.95      0.96       166\n",
            "\n",
            "    accuracy                           0.99      1115\n",
            "   macro avg       0.98      0.97      0.97      1115\n",
            "weighted avg       0.99      0.99      0.99      1115\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "n_iterations: 4\n",
            "n_required_iterations: 4\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 165\n",
            "max_resources_: 4457\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 30\n",
            "n_resources: 165\n",
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 10\n",
            "n_resources: 495\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 4\n",
            "n_resources: 1485\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 2\n",
            "n_resources: 4455\n",
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "Classifier: SVC\n",
            "Best Parameters: {'classifier__C': 100, 'classifier__gamma': 10}\n",
            "Accuracy: 0.8807174887892377\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      1.00      0.93       949\n",
            "           1       1.00      0.20      0.33       166\n",
            "\n",
            "    accuracy                           0.88      1115\n",
            "   macro avg       0.94      0.60      0.63      1115\n",
            "weighted avg       0.90      0.88      0.84      1115\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "n_iterations: 2\n",
            "n_required_iterations: 2\n",
            "n_possible_iterations: 2\n",
            "min_resources_: 1485\n",
            "max_resources_: 4457\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 7\n",
            "n_resources: 1485\n",
            "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 3\n",
            "n_resources: 4455\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Classifier: DecisionTreeClassifier\n",
            "Best Parameters: {'classifier__max_depth': 15}\n",
            "Accuracy: 0.9695067264573991\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       949\n",
            "           1       0.93      0.86      0.89       166\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.95      0.92      0.94      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "n_iterations: 3\n",
            "n_required_iterations: 3\n",
            "n_possible_iterations: 3\n",
            "min_resources_: 495\n",
            "max_resources_: 4457\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 15\n",
            "n_resources: 495\n",
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 5\n",
            "n_resources: 1485\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 2\n",
            "n_resources: 4455\n",
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "Classifier: RandomForestClassifier\n",
            "Best Parameters: {'classifier__max_features': 'sqrt', 'classifier__n_estimators': 500}\n",
            "Accuracy: 0.97847533632287\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       949\n",
            "           1       1.00      0.86      0.92       166\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.99      0.93      0.95      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Best model saved as best_model_LogisticRegression_big_data_fe_full.joblib\n",
            "Accuracy on test data using the best model: 0.9874439461883409\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       949\n",
            "           1       0.97      0.95      0.96       166\n",
            "\n",
            "    accuracy                           0.99      1115\n",
            "   macro avg       0.98      0.97      0.97      1115\n",
            "weighted avg       0.99      0.99      0.99      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VWhmN-OHpudv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}