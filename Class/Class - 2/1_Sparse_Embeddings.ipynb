{"cells":[{"cell_type":"markdown","metadata":{"id":"bzV0Las4fygR"},"source":["# <font color = 'pickle'> **Bag of Words (Sparse Embeddings)**\n"]},{"cell_type":"markdown","metadata":{"id":"aEeEQp3Pf8Kl"},"source":["<font color = 'pickle' size =5 >**What is Bag of Words (BoW)?**</font>\n","\n","A **bag-of-words** is a representation of text that describes the occurrence of words within a document <font color ='indianred'>**disregarding grammar and word order**</font>. It involves two steps:\n","\n","    1. Create Vocabulary. Each word in vocabulary forms feature(independent variable) to represent document.\n","    2. Score words (based on frequency or occurrence) to create Vectors."]},{"cell_type":"markdown","metadata":{"id":"pmUenp0Q4YDU"},"source":["<font color = 'pickle' size =5> **Why do you need to learn Bag of Words?**</font>\n","\n","- Till now we have learnt how to pre-process the text data i.e clean the text data.\n","- Our final goal is to use text data in Machine Learning (ML) models. For example - we want to predict whether e-mail is a spam or not based on the text of the data.\n","- But ML models can understand only numbers. Therefore we need to convert text to vectors (numbers).\n","- The simple method of converting text to numbers is to use 'Bag of Words approach'\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LlYmu-CkHeQb"},"source":["<font color = 'pickle' size =5> **Why do you need Bag of Words in age of LLMs?** </font>\n","\n","<font color = 'indianred'> **Outstanding paper Award ACL 2023: Linear Classifier: An Often-Forgotten Baseline for Text Classification**</font>\n","\n","*Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods.*\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kzKYz9DZ4YDU"},"source":["## <font color = 'pickle' size =5 >**Learning Outcome** </font>\n","After completing this tutorial, you will know\n","\n","1. What the bag-of-words approach is and how you can use it to represent text data.\n","2. What are different techniques to prepare a vocabulary and score words.\n","3. How to implement 'Bag-of-words' approach in python using sklearn."]},{"cell_type":"markdown","metadata":{"id":"uTmQrTrVxCEh"},"source":["# <font color = 'pickle'> **Tutorial Overview**</font>\n"," - Generating Vocab\n"," - Generating vectors using Vocab\n","     - Binary Vectorizer\n","     - Count Vectorizer\n","     - tfidf Vectorizer\n","\n"," - Modifying Vocab\n"," - Example - IMDB Dataset\n"]},{"cell_type":"markdown","metadata":{"id":"xtZUZxRsjFOZ"},"source":["# <font color = 'pickle'> Import/install Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1706504876557,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"AqC3phq7vN33","outputId":"a7a3249a-469a-4b2c-e5bb-de50e8d4095c"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"mmRrU5ZDnfGh"},"source":["The code `%load_ext autoreload` and `%autoreload 2` in a Jupyter notebook enables the autoreload extension. This setup automatically reloads imported modules before executing code cells. Specifically, `%autoreload 2` ensures all modules are reloaded each time before execution, reflecting any changes made to the module files without needing to restart the notebook kernel."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1706505454838,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"T0nri2djPi_D"},"outputs":[],"source":["import sys\n","if 'google.colab' in str(get_ipython()):\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    !pip install -U nltk -qq\n","    !pip install -U spacy -qq\n","    !pip install -U scikit-optimize -qq\n","    !python -m spacy download en_core_web_sm -qq\n","    !pip install pyspellchecker -qq\n","    !pip install optuna -qq\n","\n","    basepath = '/content/drive/MyDrive/Colab_Notebooks/BUAN_6342_Applied_Natural_Language_Processing'\n","    sys.path.append('/content/drive/MyDrive/Colab_Notebooks/BUAN_6342_Applied_Natural_Language_Processing/0_Custom_files')\n","else:\n","    basepath = '/Users/harikrishnadev/Library/CloudStorage/GoogleDrive-harikrish0607@gmail.com/My Drive/Colab_Notebooks/BUAN_6342_Applied_Natural_Language_Processing/'\n","    sys.path.append(\n","        '/Users/harikrishnadev/Library/CloudStorage/GoogleDrive-harikrish0607@gmail.com/My Drive/Colab_Notebooks/BUAN_6342_Applied_Natural_Language_Processing/0_Custom_files')"]},{"cell_type":"markdown","metadata":{"id":"Jtct-jwpoYBB"},"source":["**Code Explanation**\n","\n","1. **Environment Detection**:\n","The code begins by checking the environment in which it's running. This is done using the statement `if 'google.colab' in str(get_ipython()):`. If this condition is true, it means the code is running in Google Colab, an online interactive development environment. If not, the code assumes it's running in a local environment (like your personal computer).\n","\n","2. **Setup for Google Colab:**\n","When running in Google Colab, the code performs the following actions:\n","\n","- *Mount Google Drive*: `from google.colab import drive` and `drive.mount('/content/drive')` are used to mount the user's Google Drive to the Colab environment, enabling access to files stored there.\n","\n","- *Install Necessary Libraries*: The code installs or updates specific Python libraries (`nltk` and `spacy`) silently without producing unnecessary output (`-qq`).\n","\n","- *Download spaCy Model*: `!python -m spacy download en_core_web_sm -qq` downloads the English language model for spaCy, necessary for NLP tasks.\n","\n","- *Set Basepath and Update sys.path*:\n","  - `basepath` is set to a path in Google Drive where data or relevant files are stored.\n","  - `sys.path.append('/content/drive/MyDrive/data/custom-functions')` adds a directory to the Python search path. This allows Python to import and use custom functions located in that directory in the Colab notebook, promoting modularity and code reusability.\n","\n","3. **Setup for Local Environment:**\n","If the code is not running in Google Colab, it's presumed to be in a local environment. Here, the setup is slightly different:\n","\n","- *Set Basepath*: `basepath` is set to a specific directory on the local machine. This is where the code will look for data files or other resources.\n","\n","- *Update sys.path for Custom Functions*:\n","  - `sys.path.append('/home/harpreet/Insync/google_drive_shaannoor/data/custom-functions')` adds a local directory to the Python search path. Similar to the Colab setup, this step allows the local Python environment to import and use custom functions from the specified directory."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T20:24:56.540071Z","iopub.status.busy":"2021-09-11T20:24:56.539257Z","iopub.status.idle":"2021-09-11T20:24:56.544797Z","shell.execute_reply":"2021-09-11T20:24:56.544485Z","shell.execute_reply.started":"2021-09-11T20:24:56.539974Z"},"executionInfo":{"elapsed":106,"status":"ok","timestamp":1706505109769,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"NBkcfK5xQkjv","outputId":"f7989af0-8f01-4024-c3d0-b4955464d264","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/3d/fk1zpy415g31bg07yrkc5qbm0000gn/T/ipykernel_17072/837683949.py:2: DeprecationWarning: \n","Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n","(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n","but was not found to be installed on your system.\n","If this would cause problems for you,\n","please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n","        \n","  import pandas as pd  # For data manipulation and analysis\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/harikrishnadev/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# Import required libraries\n","import pandas as pd  # For data manipulation and analysis\n","import numpy as np   # For numerical operations\n","import spacy         # For NLP preprocessing\n","\n","# Import required nltk packages\n","import nltk\n","nltk.download('stopwords')  # Download the stopwords corpus\n","from nltk.corpus import stopwords as nltk_stopwords  # Stopwords corpus\n","\n","# Import tweet tokenizer from nltk\n","from nltk.tokenize import TweetTokenizer\n","\n","# Import CountVectorizer and TfidfVectorizer from scikit-learn\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","# Import pathlib for managing file paths\n","from pathlib import Path\n","\n","# import custom-preprocessor from python file\n","import CustomPreprocessorSpacy as cp"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1706490346600,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"t2_XksvXxCEj","outputId":"30442954-14ab-4a4d-ed26-250eb8275c5c","tags":[]},"outputs":[{"data":{"text/plain":["'3.7.2'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["spacy.__version__\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T20:24:59.919129Z","iopub.status.busy":"2021-09-11T20:24:59.918662Z","iopub.status.idle":"2021-09-11T20:25:00.179324Z","shell.execute_reply":"2021-09-11T20:25:00.178886Z","shell.execute_reply.started":"2021-09-11T20:24:59.919114Z"},"executionInfo":{"elapsed":730,"status":"ok","timestamp":1706490350164,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"PmZmCvuQQezx","tags":[]},"outputs":[],"source":["# load spacy model\n","nlp = spacy.load('en_core_web_sm')\n"]},{"cell_type":"markdown","metadata":{"id":"RTlvVAjziFYm"},"source":["## <font color = 'pickle'> **Generating Vocab**"]},{"cell_type":"markdown","metadata":{"id":"whv4QgKPB0Gv"},"source":["###  <font color = 'pickle'> **Dummy Corpus**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T20:25:03.583435Z","iopub.status.busy":"2021-09-11T20:25:03.582835Z","iopub.status.idle":"2021-09-11T20:25:03.590454Z","shell.execute_reply":"2021-09-11T20:25:03.589100Z","shell.execute_reply.started":"2021-09-11T20:25:03.583366Z"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1706506222115,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"YzXnofA3nF3W","tags":[]},"outputs":[],"source":["# Dummy corpus\n","Corpus = [\"Count Vectorizer - for this vectorizer, scoring is done based on frequency. For this vectorizer frequency is key. @vectorizer #frequency @frequency, doesnâ€™t\",\n","          \"tfidf vectorizer - for this vectorizer, scoring is done based on tfidf,  higher tfidf higher score #tfidf @vectorizer \"  ,\n","          \"Binary vectorizer - for this vectorizer, scoring is done based on presence of word. For this vectorizer, dummy is key #dummy @dummy @vectorizer \"]\n"]},{"cell_type":"markdown","metadata":{"id":"lyhgUzi5CNgg"},"source":["### <font color = 'pickle'>**Create an instance of Vectorizer**"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T20:25:17.155888Z","iopub.status.busy":"2021-09-11T20:25:17.155757Z","iopub.status.idle":"2021-09-11T20:25:17.158014Z","shell.execute_reply":"2021-09-11T20:25:17.157740Z","shell.execute_reply.started":"2021-09-11T20:25:17.155874Z"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706506223941,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"J3ZVqkIHCePq","tags":[]},"outputs":[],"source":["vectorizer = CountVectorizer()\n"]},{"cell_type":"markdown","metadata":{"id":"pIo86goBv6sg"},"source":["The above code creates an instance of the `CountVectorizer` class from the `sklearn.feature_extraction.text module`. This class is used to convert a collection of text documents to a matrix of token counts.\n","\n","It accomplishes this by\n","  1. tokenizing the input text\n","  2. creating a vocabulary of all the tokens found in the text\n","  3. encoding the text as a matrix of token counts based on this vocabulary.\n","\n","The created instance vectorizer can then be used to fit the text data to the vocabulary and generate the token count matrix."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":68,"status":"ok","timestamp":1706506226021,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"k5fgQ7Kb_9j3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mInit signature:\u001b[0m\n","\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdecode_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mpreprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'(?u)\\\\b\\\\w\\\\w+\\\\b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.int64'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSource:\u001b[0m        \n","\u001b[0;32mclass\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_VectorizerMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"Convert a collection of text documents to a matrix of token counts.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    This implementation produces a sparse representation of the counts using\u001b[0m\n","\u001b[0;34m    scipy.sparse.csr_matrix.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    If you do not provide an a-priori dictionary and you do not use an analyzer\u001b[0m\n","\u001b[0;34m    that does some kind of feature selection then the number of features will\u001b[0m\n","\u001b[0;34m    be equal to the vocabulary size found by analyzing the data.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    For an efficiency comparison of the different feature extractors, see\u001b[0m\n","\u001b[0;34m    :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    Read more in the :ref:`User Guide <text_feature_extraction>`.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    Parameters\u001b[0m\n","\u001b[0;34m    ----------\u001b[0m\n","\u001b[0;34m    input : {'filename', 'file', 'content'}, default='content'\u001b[0m\n","\u001b[0;34m        - If `'filename'`, the sequence passed as an argument to fit is\u001b[0m\n","\u001b[0;34m          expected to be a list of filenames that need reading to fetch\u001b[0m\n","\u001b[0;34m          the raw content to analyze.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        - If `'file'`, the sequence items must have a 'read' method (file-like\u001b[0m\n","\u001b[0;34m          object) that is called to fetch the bytes in memory.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        - If `'content'`, the input is expected to be a sequence of items that\u001b[0m\n","\u001b[0;34m          can be of type string or byte.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    encoding : str, default='utf-8'\u001b[0m\n","\u001b[0;34m        If bytes or files are given to analyze, this encoding is used to\u001b[0m\n","\u001b[0;34m        decode.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\u001b[0m\n","\u001b[0;34m        Instruction on what to do if a byte sequence is given to analyze that\u001b[0m\n","\u001b[0;34m        contains characters not of the given `encoding`. By default, it is\u001b[0m\n","\u001b[0;34m        'strict', meaning that a UnicodeDecodeError will be raised. Other\u001b[0m\n","\u001b[0;34m        values are 'ignore' and 'replace'.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    strip_accents : {'ascii', 'unicode'} or callable, default=None\u001b[0m\n","\u001b[0;34m        Remove accents and perform other character normalization\u001b[0m\n","\u001b[0;34m        during the preprocessing step.\u001b[0m\n","\u001b[0;34m        'ascii' is a fast method that only works on characters that have\u001b[0m\n","\u001b[0;34m        a direct ASCII mapping.\u001b[0m\n","\u001b[0;34m        'unicode' is a slightly slower method that works on any characters.\u001b[0m\n","\u001b[0;34m        None (default) means no character normalization is performed.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Both 'ascii' and 'unicode' use NFKD normalization from\u001b[0m\n","\u001b[0;34m        :func:`unicodedata.normalize`.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    lowercase : bool, default=True\u001b[0m\n","\u001b[0;34m        Convert all characters to lowercase before tokenizing.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    preprocessor : callable, default=None\u001b[0m\n","\u001b[0;34m        Override the preprocessing (strip_accents and lowercase) stage while\u001b[0m\n","\u001b[0;34m        preserving the tokenizing and n-grams generation steps.\u001b[0m\n","\u001b[0;34m        Only applies if ``analyzer`` is not callable.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    tokenizer : callable, default=None\u001b[0m\n","\u001b[0;34m        Override the string tokenization step while preserving the\u001b[0m\n","\u001b[0;34m        preprocessing and n-grams generation steps.\u001b[0m\n","\u001b[0;34m        Only applies if ``analyzer == 'word'``.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    stop_words : {'english'}, list, default=None\u001b[0m\n","\u001b[0;34m        If 'english', a built-in stop word list for English is used.\u001b[0m\n","\u001b[0;34m        There are several known issues with 'english' and you should\u001b[0m\n","\u001b[0;34m        consider an alternative (see :ref:`stop_words`).\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        If a list, that list is assumed to contain stop words, all of which\u001b[0m\n","\u001b[0;34m        will be removed from the resulting tokens.\u001b[0m\n","\u001b[0;34m        Only applies if ``analyzer == 'word'``.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        If None, no stop words will be used. In this case, setting `max_df`\u001b[0m\n","\u001b[0;34m        to a higher value, such as in the range (0.7, 1.0), can automatically detect\u001b[0m\n","\u001b[0;34m        and filter stop words based on intra corpus document frequency of terms.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\u001b[0m\n","\u001b[0;34m        Regular expression denoting what constitutes a \"token\", only used\u001b[0m\n","\u001b[0;34m        if ``analyzer == 'word'``. The default regexp select tokens of 2\u001b[0m\n","\u001b[0;34m        or more alphanumeric characters (punctuation is completely ignored\u001b[0m\n","\u001b[0;34m        and always treated as a token separator).\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        If there is a capturing group in token_pattern then the\u001b[0m\n","\u001b[0;34m        captured group content, not the entire match, becomes the token.\u001b[0m\n","\u001b[0;34m        At most one capturing group is permitted.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    ngram_range : tuple (min_n, max_n), default=(1, 1)\u001b[0m\n","\u001b[0;34m        The lower and upper boundary of the range of n-values for different\u001b[0m\n","\u001b[0;34m        word n-grams or char n-grams to be extracted. All values of n such\u001b[0m\n","\u001b[0;34m        such that min_n <= n <= max_n will be used. For example an\u001b[0m\n","\u001b[0;34m        ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\u001b[0m\n","\u001b[0;34m        unigrams and bigrams, and ``(2, 2)`` means only bigrams.\u001b[0m\n","\u001b[0;34m        Only applies if ``analyzer`` is not callable.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\u001b[0m\n","\u001b[0;34m        Whether the feature should be made of word n-gram or character\u001b[0m\n","\u001b[0;34m        n-grams.\u001b[0m\n","\u001b[0;34m        Option 'char_wb' creates character n-grams only from text inside\u001b[0m\n","\u001b[0;34m        word boundaries; n-grams at the edges of words are padded with space.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        If a callable is passed it is used to extract the sequence of features\u001b[0m\n","\u001b[0;34m        out of the raw, unprocessed input.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        .. versionchanged:: 0.21\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\u001b[0m\n","\u001b[0;34m        first read from the file and then passed to the given callable\u001b[0m\n","\u001b[0;34m        analyzer.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    max_df : float in range [0.0, 1.0] or int, default=1.0\u001b[0m\n","\u001b[0;34m        When building the vocabulary ignore terms that have a document\u001b[0m\n","\u001b[0;34m        frequency strictly higher than the given threshold (corpus-specific\u001b[0m\n","\u001b[0;34m        stop words).\u001b[0m\n","\u001b[0;34m        If float, the parameter represents a proportion of documents, integer\u001b[0m\n","\u001b[0;34m        absolute counts.\u001b[0m\n","\u001b[0;34m        This parameter is ignored if vocabulary is not None.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    min_df : float in range [0.0, 1.0] or int, default=1\u001b[0m\n","\u001b[0;34m        When building the vocabulary ignore terms that have a document\u001b[0m\n","\u001b[0;34m        frequency strictly lower than the given threshold. This value is also\u001b[0m\n","\u001b[0;34m        called cut-off in the literature.\u001b[0m\n","\u001b[0;34m        If float, the parameter represents a proportion of documents, integer\u001b[0m\n","\u001b[0;34m        absolute counts.\u001b[0m\n","\u001b[0;34m        This parameter is ignored if vocabulary is not None.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    max_features : int, default=None\u001b[0m\n","\u001b[0;34m        If not None, build a vocabulary that only consider the top\u001b[0m\n","\u001b[0;34m        `max_features` ordered by term frequency across the corpus.\u001b[0m\n","\u001b[0;34m        Otherwise, all features are used.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        This parameter is ignored if vocabulary is not None.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    vocabulary : Mapping or iterable, default=None\u001b[0m\n","\u001b[0;34m        Either a Mapping (e.g., a dict) where keys are terms and values are\u001b[0m\n","\u001b[0;34m        indices in the feature matrix, or an iterable over terms. If not\u001b[0m\n","\u001b[0;34m        given, a vocabulary is determined from the input documents. Indices\u001b[0m\n","\u001b[0;34m        in the mapping should not be repeated and should not have any gap\u001b[0m\n","\u001b[0;34m        between 0 and the largest index.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    binary : bool, default=False\u001b[0m\n","\u001b[0;34m        If True, all non zero counts are set to 1. This is useful for discrete\u001b[0m\n","\u001b[0;34m        probabilistic models that model binary events rather than integer\u001b[0m\n","\u001b[0;34m        counts.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    dtype : dtype, default=np.int64\u001b[0m\n","\u001b[0;34m        Type of the matrix returned by fit_transform() or transform().\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    Attributes\u001b[0m\n","\u001b[0;34m    ----------\u001b[0m\n","\u001b[0;34m    vocabulary_ : dict\u001b[0m\n","\u001b[0;34m        A mapping of terms to feature indices.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    fixed_vocabulary_ : bool\u001b[0m\n","\u001b[0;34m        True if a fixed vocabulary of term to indices mapping\u001b[0m\n","\u001b[0;34m        is provided by the user.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    stop_words_ : set\u001b[0m\n","\u001b[0;34m        Terms that were ignored because they either:\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m          - occurred in too many documents (`max_df`)\u001b[0m\n","\u001b[0;34m          - occurred in too few documents (`min_df`)\u001b[0m\n","\u001b[0;34m          - were cut off by feature selection (`max_features`).\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        This is only available if no vocabulary was given.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    See Also\u001b[0m\n","\u001b[0;34m    --------\u001b[0m\n","\u001b[0;34m    HashingVectorizer : Convert a collection of text documents to a\u001b[0m\n","\u001b[0;34m        matrix of token counts.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    TfidfVectorizer : Convert a collection of raw documents to a matrix\u001b[0m\n","\u001b[0;34m        of TF-IDF features.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    Notes\u001b[0m\n","\u001b[0;34m    -----\u001b[0m\n","\u001b[0;34m    The ``stop_words_`` attribute can get large and increase the model size\u001b[0m\n","\u001b[0;34m    when pickling. This attribute is provided only for introspection and can\u001b[0m\n","\u001b[0;34m    be safely removed using delattr or set to None before pickling.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    Examples\u001b[0m\n","\u001b[0;34m    --------\u001b[0m\n","\u001b[0;34m    >>> from sklearn.feature_extraction.text import CountVectorizer\u001b[0m\n","\u001b[0;34m    >>> corpus = [\u001b[0m\n","\u001b[0;34m    ...     'This is the first document.',\u001b[0m\n","\u001b[0;34m    ...     'This document is the second document.',\u001b[0m\n","\u001b[0;34m    ...     'And this is the third one.',\u001b[0m\n","\u001b[0;34m    ...     'Is this the first document?',\u001b[0m\n","\u001b[0;34m    ... ]\u001b[0m\n","\u001b[0;34m    >>> vectorizer = CountVectorizer()\u001b[0m\n","\u001b[0;34m    >>> X = vectorizer.fit_transform(corpus)\u001b[0m\n","\u001b[0;34m    >>> vectorizer.get_feature_names_out()\u001b[0m\n","\u001b[0;34m    array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\u001b[0m\n","\u001b[0;34m           'this'], ...)\u001b[0m\n","\u001b[0;34m    >>> print(X.toarray())\u001b[0m\n","\u001b[0;34m    [[0 1 1 1 0 0 1 0 1]\u001b[0m\n","\u001b[0;34m     [0 2 0 1 0 1 1 0 1]\u001b[0m\n","\u001b[0;34m     [1 0 0 1 1 0 1 1 1]\u001b[0m\n","\u001b[0;34m     [0 1 1 1 0 0 1 0 1]]\u001b[0m\n","\u001b[0;34m    >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\u001b[0m\n","\u001b[0;34m    >>> X2 = vectorizer2.fit_transform(corpus)\u001b[0m\n","\u001b[0;34m    >>> vectorizer2.get_feature_names_out()\u001b[0m\n","\u001b[0;34m    array(['and this', 'document is', 'first document', 'is the', 'is this',\u001b[0m\n","\u001b[0;34m           'second document', 'the first', 'the second', 'the third', 'third one',\u001b[0m\n","\u001b[0;34m           'this document', 'this is', 'this the'], ...)\u001b[0m\n","\u001b[0;34m     >>> print(X2.toarray())\u001b[0m\n","\u001b[0;34m     [[0 0 1 1 0 0 1 0 0 0 0 1 0]\u001b[0m\n","\u001b[0;34m     [0 1 0 1 0 1 0 1 0 0 1 0 0]\u001b[0m\n","\u001b[0;34m     [1 0 0 1 0 0 0 0 1 1 0 1 0]\u001b[0m\n","\u001b[0;34m     [0 0 1 0 1 0 1 0 0 0 0 0 1]]\u001b[0m\n","\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"filename\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"decode_error\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"strip_accents\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unicode\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"lowercase\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"boolean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"preprocessor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"tokenizer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"stop_words\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"token_pattern\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"ngram_range\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"analyzer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"char\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"char_wb\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"max_df\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mInterval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRealNotInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"both\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mInterval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"min_df\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mInterval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRealNotInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"both\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mInterval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"max_features\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mInterval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"vocabulary\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHasMethods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"boolean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"no_validation\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# delegate to numpy\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mdecode_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mpreprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr\"(?u)\\b\\w\\w+\\b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_error\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_accents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Sort features by name\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Returns a reordered matrix and modifies the vocabulary in place\u001b[0m\n","\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0msorted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mmap_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mmap_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"clip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_limit_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Remove too rare or too common features.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Prune features that are non zero in more samples than high or less\u001b[0m\n","\u001b[0;34m        documents than low, modifying the vocabulary, and restricting it to\u001b[0m\n","\u001b[0;34m        at most the limit most frequent.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        This does not prune samples with zero features.\u001b[0m\n","\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mhigh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# Calculate a mask based on document frequencies\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_document_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mhigh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mmask\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0mdfs\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mmask\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0mdfs\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mtfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mmask_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mnew_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mnew_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mask\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mnew_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# maps old indices to new\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mremoved_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;32mdel\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mremoved_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mkept_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkept_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;34m\"After pruning, no terms remain. Try a lower min_df or a higher max_df.\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkept_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_terms\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;31m# Add a new value when a new vocabulary item is seen\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0manalyze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mj_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_int_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                    \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                        \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                        \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                    \u001b[0;31m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                    \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;31m# disable defaultdict behaviour\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                    \u001b[0;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# = 2**31 - 1\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                    \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                        \u001b[0;34m\"sparse CSR array has {} non-zero \"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                        \u001b[0;34m\"elements and requires 64 bit indexing, \"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                        \u001b[0;34m\"which is unsupported with 32 bit Python.\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                    \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mindices_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mindices_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mj_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Parameters\u001b[0m\n","\u001b[0;34m        ----------\u001b[0m\n","\u001b[0;34m        raw_documents : iterable\u001b[0m\n","\u001b[0;34m            An iterable which generates either str, unicode or file objects.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        y : None\u001b[0m\n","\u001b[0;34m            This parameter is ignored.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Returns\u001b[0m\n","\u001b[0;34m        -------\u001b[0m\n","\u001b[0;34m        self : object\u001b[0m\n","\u001b[0;34m            Fitted vectorizer.\u001b[0m\n","\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Learn the vocabulary dictionary and return document-term matrix.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        This is equivalent to fit followed by transform, but more efficiently\u001b[0m\n","\u001b[0;34m        implemented.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Parameters\u001b[0m\n","\u001b[0;34m        ----------\u001b[0m\n","\u001b[0;34m        raw_documents : iterable\u001b[0m\n","\u001b[0;34m            An iterable which generates either str, unicode or file objects.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        y : None\u001b[0m\n","\u001b[0;34m            This parameter is ignored.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Returns\u001b[0m\n","\u001b[0;34m        -------\u001b[0m\n","\u001b[0;34m        X : array of shape (n_samples, n_features)\u001b[0m\n","\u001b[0;34m            Document-term matrix.\u001b[0m\n","\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# We intentionally don't call the transform method to make\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# fit_transform overridable without unwanted side effects in\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# TfidfVectorizer.\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_ngram_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mmin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                    \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                        \u001b[0;34m\"Upper case characters found in\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                        \u001b[0;34m\" vocabulary while 'lowercase'\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                        \u001b[0;34m\" is True. These entries will not\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                        \u001b[0;34m\" be matched with any documents\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                    \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mn_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mmax_doc_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_doc\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mmin_doc_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_doc\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mmax_doc_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_df corresponds to < documents than min_df\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_limit_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Transform documents to document-term matrix.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Extract token counts out of raw text documents using the vocabulary\u001b[0m\n","\u001b[0;34m        fitted with fit or the one provided to the constructor.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Parameters\u001b[0m\n","\u001b[0;34m        ----------\u001b[0m\n","\u001b[0;34m        raw_documents : iterable\u001b[0m\n","\u001b[0;34m            An iterable which generates either str, unicode or file objects.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Returns\u001b[0m\n","\u001b[0;34m        -------\u001b[0m\n","\u001b[0;34m        X : sparse matrix of shape (n_samples, n_features)\u001b[0m\n","\u001b[0;34m            Document-term matrix.\u001b[0m\n","\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Return terms per document with nonzero entries in X.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Parameters\u001b[0m\n","\u001b[0;34m        ----------\u001b[0m\n","\u001b[0;34m        X : {array-like, sparse matrix} of shape (n_samples, n_features)\u001b[0m\n","\u001b[0;34m            Document-term matrix.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Returns\u001b[0m\n","\u001b[0;34m        -------\u001b[0m\n","\u001b[0;34m        X_inv : list of arrays of shape (n_samples,)\u001b[0m\n","\u001b[0;34m            List of arrays of terms.\u001b[0m\n","\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# We need CSR format for fast row manipulations.\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0minverse_vocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0minverse_vocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0minverse_vocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get output feature names for transformation.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Parameters\u001b[0m\n","\u001b[0;34m        ----------\u001b[0m\n","\u001b[0;34m        input_features : array-like of str or None, default=None\u001b[0m\n","\u001b[0;34m            Not used, present here for API consistency by convention.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m        Returns\u001b[0m\n","\u001b[0;34m        -------\u001b[0m\n","\u001b[0;34m        feature_names_out : ndarray of str objects\u001b[0m\n","\u001b[0;34m            Transformed feature names.\u001b[0m\n","\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"X_types\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFile:\u001b[0m           /opt/homebrew/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\n","\u001b[0;31mType:\u001b[0m           type\n","\u001b[0;31mSubclasses:\u001b[0m     TfidfVectorizer"]}],"source":["CountVectorizer??\n"]},{"cell_type":"markdown","metadata":{"id":"u6kX6-YQCSuq"},"source":["### <font color = 'pickle'>**Fit Vectorizer on corpus to generate vocab**"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1706506230806,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"fnehlHO7RQKx","outputId":"58c651c8-4b80-4ef4-d76b-6c58e76ff7e0","tags":[]},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"â–¸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"â–¾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer()</pre></div> </div></div></div></div>"],"text/plain":["CountVectorizer()"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Fit the vectorizer on corpus\n","vectorizer.fit(Corpus)\n"]},{"cell_type":"markdown","metadata":{"id":"R-8UMn_FSNCF"},"source":["<font color = 'indianred'>**Vectorizer().fit() does the following**:\n","- lowercases your text\n","- uses utf-8 encoding\n","- performs tokenization (converts raw text to smaller units of text)\n","- uses word level tokenization (meaning each word is treated as a separate token) and  ignores single characters during tokenization ( words like â€˜aâ€™ and â€˜Iâ€™ are removed)\n","- By default, the regular expression that is used to split the text and create tokens is : `\"\\b\\w\\w+\\b\"`.\n","  - This means it finds all sequences of characters that consist of at least two letters or numbers(\\w) and that are separated by word boundaries (\\b).\n","  - It does not find single-letter words, and it splits up contractions like â€œdoesnâ€™tâ€ or â€œbit.lyâ€, but it matches â€œh8terâ€ as a single word.\n","- The CountVectorizer then converts all words to lowercasecharacters, so that â€œsoonâ€, â€œSoonâ€, and â€œsOonâ€ all correspond to the same token (and therefore feature).\n","- It then creates a dictionary of unique words.\n","- The set of unique words is used as features in the CountVectorizer."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1706506233001,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"ayvQssZBRTNr","outputId":"89a4ff52-0e8d-40f7-9ec0-4636531be557","tags":[]},"outputs":[{"data":{"text/plain":["{'count': 2,\n"," 'vectorizer': 18,\n"," 'for': 6,\n"," 'this': 17,\n"," 'scoring': 15,\n"," 'is': 9,\n"," 'done': 4,\n"," 'based': 0,\n"," 'on': 12,\n"," 'frequency': 7,\n"," 'key': 10,\n"," 'doesn': 3,\n"," 'tfidf': 16,\n"," 'higher': 8,\n"," 'score': 14,\n"," 'binary': 1,\n"," 'presence': 13,\n"," 'of': 11,\n"," 'word': 19,\n"," 'dummy': 5}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Display the mapping of terms to feature indices created by the vectorizer\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"WAsQM4qer-nx"},"source":["We can use `vectorizer.get_feature_names_out()` to obtain the list of unique words that the CountVectorizer has identified from the corpus"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1706506236763,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"4Z1haa0zRkXM","outputId":"d641ab76-3df8-488c-f5c0-955d7dbb0299","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['based' 'binary' 'count' 'doesn' 'done' 'dummy' 'for' 'frequency'\n"," 'higher' 'is' 'key' 'of' 'on' 'presence' 'score' 'scoring' 'tfidf' 'this'\n"," 'vectorizer' 'word']\n","\n","total number of unique features (words): 20\n"]}],"source":["# Retrieve the list of unique words (tokens) that the CountVectorizer identified in the corpus.\n","# These words serve as features (columns) in the vectorized output.\n","features = vectorizer.get_feature_names_out()\n","print(features)\n","\n","# Print the total number of unique features (words) identified in the corpus\n","print('\\ntotal number of unique features (words):', len(features))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mP8odLcIC66i"},"source":["## <font color = 'pickle'>**Generate Vectors using Vocab**"]},{"cell_type":"markdown","metadata":{"id":"9DwforYaDNsY"},"source":["### <font color = 'pickle'>**Binary Vectorizer**"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1706506240137,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"5x5YgOMdDiYC","outputId":"eadcef2a-0ea2-468a-dde6-2ef73c037241","tags":[]},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-2 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-2 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-2 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-2 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-2 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-2 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-2 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-2 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-2 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-2 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-2 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"â–¸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-2 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"â–¾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-2 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-2 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-2 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-2 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-2 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-2 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-2 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-2 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer(binary=True)</pre></div> </div></div></div></div>"],"text/plain":["CountVectorizer(binary=True)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["binary_vectorizer = CountVectorizer(binary=True)\n","binary_vectorizer.fit(Corpus)\n"]},{"cell_type":"markdown","metadata":{"id":"X4k8F2UTE4SU"},"source":["- We can now call transform() method to transform documents in our corpus to vectors.\n","- <font color = 'dodgerblue'>**Each document**</font> will be represented by <font color = 'dodgerblue'>**vector of length equal to len(dictionary)**.</font>\n","- The vectors are stored in the form of a <font color = 'dodgerblue'>**sparse matrix**.</font>\n","- We can use <font color = 'dodgerblue'>**toarray()**</font> function to get complete matrix.\n","- Number of columns represent the number of features (len(vocab)).\n","- Number of rows represent the number the documents in a corpus.\n","- <font color = 'dodgerblue'>**For each row, the numbers displayed are 0 or 1 - indicating absence or presence of a word in a document.**"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["{'count': 2,\n"," 'vectorizer': 18,\n"," 'for': 6,\n"," 'this': 17,\n"," 'scoring': 15,\n"," 'is': 9,\n"," 'done': 4,\n"," 'based': 0,\n"," 'on': 12,\n"," 'frequency': 7,\n"," 'key': 10,\n"," 'doesn': 3,\n"," 'tfidf': 16,\n"," 'higher': 8,\n"," 'score': 14,\n"," 'binary': 1,\n"," 'presence': 13,\n"," 'of': 11,\n"," 'word': 19,\n"," 'dummy': 5}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["binary_vectorizer.vocabulary_"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T20:25:24.312760Z","iopub.status.busy":"2021-09-11T20:25:24.312601Z","iopub.status.idle":"2021-09-11T20:25:24.315437Z","shell.execute_reply":"2021-09-11T20:25:24.315041Z","shell.execute_reply.started":"2021-09-11T20:25:24.312747Z"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1706506242654,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"DzwuRnsInKHa","tags":[]},"outputs":[],"source":["binary_vectors = binary_vectorizer.transform(Corpus)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1706506243314,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"kfXmM7P3nVYy","outputId":"bbafb524-447a-4602-c50e-0b22c6268e6f","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["vectors in sparse format\n","  (0, 0)\t1\n","  (0, 2)\t1\n","  (0, 3)\t1\n","  (0, 4)\t1\n","  (0, 6)\t1\n","  (0, 7)\t1\n","  (0, 9)\t1\n","  (0, 10)\t1\n","  (0, 12)\t1\n","  (0, 15)\t1\n","  (0, 17)\t1\n","  (0, 18)\t1\n","  (1, 0)\t1\n","  (1, 4)\t1\n","  (1, 6)\t1\n","  (1, 8)\t1\n","  (1, 9)\t1\n","  (1, 12)\t1\n","  (1, 14)\t1\n","  (1, 15)\t1\n","  (1, 16)\t1\n","  (1, 17)\t1\n","  (1, 18)\t1\n","  (2, 0)\t1\n","  (2, 1)\t1\n","  (2, 4)\t1\n","  (2, 5)\t1\n","  (2, 6)\t1\n","  (2, 9)\t1\n","  (2, 10)\t1\n","  (2, 11)\t1\n","  (2, 12)\t1\n","  (2, 13)\t1\n","  (2, 15)\t1\n","  (2, 17)\t1\n","  (2, 18)\t1\n","  (2, 19)\t1\n"]}],"source":["print(f'vectors in sparse format')\n","print(binary_vectors)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1706506245580,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"pr4aHet7Z3t_","outputId":"171feb19-c0ef-4c3d-f03b-672d4307461a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","binary vectors in array(dense) format\n","[[1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0]\n"," [1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0]\n"," [1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1]]\n","\n","The shape of the binary vectors is : (3, 20)\n"]}],"source":["print(f'\\nbinary vectors in array(dense) format')\n","print(binary_vectors.toarray())\n","print(\n","    f'\\nThe shape of the binary vectors is : {binary_vectors.toarray().shape}')\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1706506247403,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"3otH8XphHQJX","outputId":"78b14186-eef5-4e3f-e91a-c79b678b0690"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>based</th>\n","      <th>binary</th>\n","      <th>count</th>\n","      <th>doesn</th>\n","      <th>done</th>\n","      <th>dummy</th>\n","      <th>for</th>\n","      <th>frequency</th>\n","      <th>higher</th>\n","      <th>is</th>\n","      <th>key</th>\n","      <th>of</th>\n","      <th>on</th>\n","      <th>presence</th>\n","      <th>score</th>\n","      <th>scoring</th>\n","      <th>tfidf</th>\n","      <th>this</th>\n","      <th>vectorizer</th>\n","      <th>word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   based  binary  count  doesn  done  dummy  for  frequency  higher  is  key  \\\n","0      1       0      1      1     1      0    1          1       0   1    1   \n","1      1       0      0      0     1      0    1          0       1   1    0   \n","2      1       1      0      0     1      1    1          0       0   1    1   \n","\n","   of  on  presence  score  scoring  tfidf  this  vectorizer  word  \n","0   0   1         0      0        1      0     1           1     0  \n","1   0   1         0      1        1      1     1           1     0  \n","2   1   1         1      0        1      0     1           1     1  "]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# create dataframe for better visualization\n","df_binary = pd.DataFrame(binary_vectors.toarray(), columns=features)\n","df_binary\n"]},{"cell_type":"markdown","metadata":{"id":"OAPPPILCATX-"},"source":["### <font color = 'pickle'>**Count Vectorizer**\n","-  The vectors are stored in the form of a sparse matrix.\n","- Number of columns represent the number of features (len(vocab))\n","- Number of rows represent the number the documents in a corpus\n","- Thus, each document is represented by a vector of size of length of vocab.\n","- For each row, <font color = 'dodgerblue'>**the numbers displayed are the number of times a particular word has occurred in the document.**"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1706506269985,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"PuF8IkLUA8zp","outputId":"0a1b28c8-64e2-45c9-92b4-bc5d6de73571","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["count vectors in array (dense) format\n","\n","[[1 0 1 1 1 0 2 4 0 2 1 0 1 0 0 1 0 2 4 0]\n"," [1 0 0 0 1 0 1 0 2 1 0 0 1 0 1 1 4 1 3 0]\n"," [1 1 0 0 1 3 2 0 0 2 1 1 1 1 0 1 0 2 4 1]]\n","\n","The shape of the count vectors is : (3, 20)\n"]}],"source":["term_freq_vectorizer = CountVectorizer(binary=False)\n","# we can combine fit and transform steps into a single step using fit_transform()\n","count_vectors = term_freq_vectorizer.fit_transform(Corpus)\n","print(f'count vectors in array (dense) format\\n')\n","print(count_vectors.toarray())\n","print(f'\\nThe shape of the count vectors is : {count_vectors.toarray().shape}')\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1706506275687,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"DLJCKNMVH0nm","outputId":"c67e93a7-4a73-44b1-a267-c91c77e109b5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>based</th>\n","      <th>binary</th>\n","      <th>count</th>\n","      <th>doesn</th>\n","      <th>done</th>\n","      <th>dummy</th>\n","      <th>for</th>\n","      <th>frequency</th>\n","      <th>higher</th>\n","      <th>is</th>\n","      <th>key</th>\n","      <th>of</th>\n","      <th>on</th>\n","      <th>presence</th>\n","      <th>score</th>\n","      <th>scoring</th>\n","      <th>tfidf</th>\n","      <th>this</th>\n","      <th>vectorizer</th>\n","      <th>word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   based  binary  count  doesn  done  dummy  for  frequency  higher  is  key  \\\n","0      1       0      1      1     1      0    2          4       0   2    1   \n","1      1       0      0      0     1      0    1          0       2   1    0   \n","2      1       1      0      0     1      3    2          0       0   2    1   \n","\n","   of  on  presence  score  scoring  tfidf  this  vectorizer  word  \n","0   0   1         0      0        1      0     2           4     0  \n","1   0   1         0      1        1      4     1           3     0  \n","2   1   1         1      0        1      0     2           4     1  "]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# create dataframe for better visualization\n","df_count = pd.DataFrame(count_vectors.toarray(),\n","                        columns=term_freq_vectorizer.get_feature_names_out())\n","df_count\n"]},{"cell_type":"markdown","metadata":{"id":"dVlJmMtmhCYM"},"source":["### <font color = 'pickle'>**tf-idf Vectorizer**</font>\n","\n","- One measure of how important a word is term frequency (tf) (how frequently a word occurs in a document). We examined term frequency in previous sections where we used CountVectorizer to get the freqency of each word.\n","- But there may be words in a document, that occur many times but these words also occur in all other documents as well.\n","- Therefore the word might not be a good representation of the document.\n","- We can account for this by  <font color = 'dodgerblue'>giving more importance to words that occur in fewer documents using inverse document frequency </font> ((# Number of documents) / (Number of documents containing the word)).\n","- This can be <font color = 'dodgerblue'>combined with term frequency</font> to calculate a termâ€™s tf-idf (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used.\n","- The idea of tf-idf is to <font color = 'dodgerblue'>find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents.</font>\n","- tf-idf gives more weight to the the words that are important (i.e., occur more frequently) in a given document, but occur rarely in other documents."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1706506280432,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"hmb0UrD_xCEo","outputId":"f995b22d-e940-4e23-bac3-8e9ac244867b","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["tfidf vectors in array (dense) format\n","\n","[[0.10829999 0.         0.18336782 0.18336782 0.10829999 0.\n","  0.21659998 0.73347128 0.         0.21659998 0.13945595 0.\n","  0.10829999 0.         0.         0.10829999 0.         0.21659998\n","  0.43319995 0.        ]\n"," [0.11455596 0.         0.         0.         0.11455596 0.\n","  0.11455596 0.         0.3879202  0.11455596 0.         0.\n","  0.11455596 0.         0.1939601  0.11455596 0.77584039 0.11455596\n","  0.34366788 0.        ]\n"," [0.11874019 0.20104462 0.         0.         0.11874019 0.60313387\n","  0.23748039 0.         0.         0.23748039 0.15289962 0.20104462\n","  0.11874019 0.20104462 0.         0.11874019 0.         0.23748039\n","  0.47496077 0.20104462]]\n","\n","The shape of the tfidf vectors is : (3, 20)\n"]}],"source":["tfidf_vectorizer = TfidfVectorizer()\n","# we can combine fit and transform steps into a single step using fit_transform()\n","tfidf_vectors = tfidf_vectorizer.fit_transform(Corpus)\n","print(f'tfidf vectors in array (dense) format\\n')\n","print(tfidf_vectors.toarray())\n","print(f'\\nThe shape of the tfidf vectors is : {tfidf_vectors.toarray().shape}')\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1706506283382,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"rz8SIqG_IZo1","outputId":"1408474d-2384-4b8e-992d-f673106e70c6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>based</th>\n","      <th>binary</th>\n","      <th>count</th>\n","      <th>doesn</th>\n","      <th>done</th>\n","      <th>dummy</th>\n","      <th>for</th>\n","      <th>frequency</th>\n","      <th>higher</th>\n","      <th>is</th>\n","      <th>key</th>\n","      <th>of</th>\n","      <th>on</th>\n","      <th>presence</th>\n","      <th>score</th>\n","      <th>scoring</th>\n","      <th>tfidf</th>\n","      <th>this</th>\n","      <th>vectorizer</th>\n","      <th>word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.1083</td>\n","      <td>0.000</td>\n","      <td>0.1834</td>\n","      <td>0.1834</td>\n","      <td>0.1083</td>\n","      <td>0.0000</td>\n","      <td>0.2166</td>\n","      <td>0.7335</td>\n","      <td>0.0000</td>\n","      <td>0.2166</td>\n","      <td>0.1395</td>\n","      <td>0.000</td>\n","      <td>0.1083</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0.1083</td>\n","      <td>0.0000</td>\n","      <td>0.2166</td>\n","      <td>0.4332</td>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.1146</td>\n","      <td>0.000</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.1146</td>\n","      <td>0.0000</td>\n","      <td>0.1146</td>\n","      <td>0.0000</td>\n","      <td>0.3879</td>\n","      <td>0.1146</td>\n","      <td>0.0000</td>\n","      <td>0.000</td>\n","      <td>0.1146</td>\n","      <td>0.000</td>\n","      <td>0.194</td>\n","      <td>0.1146</td>\n","      <td>0.7758</td>\n","      <td>0.1146</td>\n","      <td>0.3437</td>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.1187</td>\n","      <td>0.201</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.1187</td>\n","      <td>0.6031</td>\n","      <td>0.2375</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.2375</td>\n","      <td>0.1529</td>\n","      <td>0.201</td>\n","      <td>0.1187</td>\n","      <td>0.201</td>\n","      <td>0.000</td>\n","      <td>0.1187</td>\n","      <td>0.0000</td>\n","      <td>0.2375</td>\n","      <td>0.4750</td>\n","      <td>0.201</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    based  binary   count   doesn    done   dummy     for  frequency  higher  \\\n","0  0.1083   0.000  0.1834  0.1834  0.1083  0.0000  0.2166     0.7335  0.0000   \n","1  0.1146   0.000  0.0000  0.0000  0.1146  0.0000  0.1146     0.0000  0.3879   \n","2  0.1187   0.201  0.0000  0.0000  0.1187  0.6031  0.2375     0.0000  0.0000   \n","\n","       is     key     of      on  presence  score  scoring   tfidf    this  \\\n","0  0.2166  0.1395  0.000  0.1083     0.000  0.000   0.1083  0.0000  0.2166   \n","1  0.1146  0.0000  0.000  0.1146     0.000  0.194   0.1146  0.7758  0.1146   \n","2  0.2375  0.1529  0.201  0.1187     0.201  0.000   0.1187  0.0000  0.2375   \n","\n","   vectorizer   word  \n","0      0.4332  0.000  \n","1      0.3437  0.000  \n","2      0.4750  0.201  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# create dataframe for better visualization\n","df_tfidf = pd.DataFrame(tfidf_vectors.toarray(),\n","                        columns=tfidf_vectorizer.get_feature_names_out())\n","df_tfidf.round(4)\n"]},{"cell_type":"markdown","metadata":{"id":"nq8pvYnkxCEo"},"source":["### <font color = 'pickle'>**Undertstanding tfidf calculations**"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2021-09-11T20:59:13.152712Z","iopub.status.busy":"2021-09-11T20:59:13.152444Z","iopub.status.idle":"2021-09-11T20:59:13.158508Z","shell.execute_reply":"2021-09-11T20:59:13.157542Z","shell.execute_reply.started":"2021-09-11T20:59:13.152683Z"},"id":"wSoWEk3DxCEo","tags":[]},"source":["By default <br>\n","$\\text{tfidf}(w, d) = \\text{tf(w, d)} * \\text{idf(w)}$\n","<br>\n","$\\text{idf(w)} = \\log\\big(\\frac{N + 1}{N_w + 1}\\big) + 1$\n","<br><br>\n","if smooth_idf = False (default is True):\n","<br>\n","$\\text{idf(w)} = \\log\\big(\\frac{N }{N_w}\\big) + 1$\n","<br><br>\n","if sublinear_tfbool = True (default is False)\n","<br>\n","$\\text{tf(w, d)} = \\log(\\text{tf(w, d)} ) + 1$\n","\n","Here:<br>\n","- $\\text{tf}(w, d)$ is number of times word $w$ appears in document $d$\n","<br>\n","- $\\text{idf}(w)$ is inverse document frequency of word $w$\n","- $N$ is total number of documents\n","- $N_w$ is number of documents that contain word w"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1706506294812,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"fuTBzPljxCEo","outputId":"46fee6b0-813f-4586-8397-c1722e91926b","tags":[]},"outputs":[{"data":{"text/plain":["array([1.        , 1.69314718, 1.69314718, 1.69314718, 1.        ,\n","       1.69314718, 1.        , 1.69314718, 1.69314718, 1.        ,\n","       1.28768207, 1.69314718, 1.        , 1.69314718, 1.69314718,\n","       1.        , 1.69314718, 1.        , 1.        , 1.69314718])"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Calculate inverse document frequency for each feature (word)\n","term_idf = tfidf_vectorizer.idf_\n","term_idf\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1706506298053,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"GnwOVSQbJhVf","outputId":"ac3f63d5-8fae-4769-f1ed-6b1dea646eb7"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>based</th>\n","      <th>binary</th>\n","      <th>count</th>\n","      <th>doesn</th>\n","      <th>done</th>\n","      <th>dummy</th>\n","      <th>for</th>\n","      <th>frequency</th>\n","      <th>higher</th>\n","      <th>is</th>\n","      <th>key</th>\n","      <th>of</th>\n","      <th>on</th>\n","      <th>presence</th>\n","      <th>score</th>\n","      <th>scoring</th>\n","      <th>tfidf</th>\n","      <th>this</th>\n","      <th>vectorizer</th>\n","      <th>word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","      <td>1.6931</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.2877</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.6931</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   based  binary   count   doesn  done   dummy  for  frequency  higher   is  \\\n","0    1.0  1.6931  1.6931  1.6931   1.0  1.6931  1.0     1.6931  1.6931  1.0   \n","\n","      key      of   on  presence   score  scoring   tfidf  this  vectorizer  \\\n","0  1.2877  1.6931  1.0    1.6931  1.6931      1.0  1.6931   1.0         1.0   \n","\n","     word  \n","0  1.6931  "]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# create dataframe for better visualization\n","df_idf = pd.DataFrame(term_idf, index=tfidf_vectorizer.get_feature_names_out())\n","df_idf.round(4).T"]},{"cell_type":"markdown","metadata":{"id":"x_tR9I6wzEe9"},"source":["For better understanding, we will now just look at the first document and get the idf and term frequencies for each word in the first document"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1706491209235,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"CGXv36hp6wIk","outputId":"1cf94aee-d416-493b-807a-f04e6d15f8d1"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>features</th>\n","      <th>tf</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>based</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>binary</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>count</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>doesn</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>done</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>dummy</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>for</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>frequency</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>higher</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>is</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>key</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>of</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>on</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>presence</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>score</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>scoring</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>tfidf</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>this</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>vectorizer</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>word</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      features  tf\n","0        based   1\n","1       binary   0\n","2        count   1\n","3        doesn   1\n","4         done   1\n","5        dummy   0\n","6          for   2\n","7    frequency   4\n","8       higher   0\n","9           is   2\n","10         key   1\n","11          of   0\n","12          on   1\n","13    presence   0\n","14       score   0\n","15     scoring   1\n","16       tfidf   0\n","17        this   2\n","18  vectorizer   4\n","19        word   0"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# create dataframe for tf vectors for the first document\n","\n","# Create a dense numpy array from the sparse count vector for the first document\n","first_document_tf = count_vectors[0].toarray().ravel()\n","\n","# Get the feature names for the term frequency vectors\n","feature_names_tf = term_freq_vectorizer.get_feature_names_out()\n","\n","# Create a dataframe from the term frequency feature names and values\n","df_tf = pd.DataFrame({'features': feature_names_tf, 'tf': first_document_tf})\n","df_tf\n"]},{"cell_type":"markdown","metadata":{"id":"8MUwbSGf-C3V"},"source":["Note: The `toarray` method is used to convert the sparse matrix into a dense numpy array, and `ravel` is used to flatten the resulting 2-dimensional array into a 1-dimensional array. This is necessary because pandas dataframes expect 1-dimensional arrays as values for the columns."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1706506321984,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"qmWgJ7RP-AgQ","outputId":"5b7b7d97-fe12-4a09-9533-954e1ccb7e0e"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>features</th>\n","      <th>tf</th>\n","      <th>idf</th>\n","      <th>norm_tfidf</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7</th>\n","      <td>frequency</td>\n","      <td>4</td>\n","      <td>1.693147</td>\n","      <td>0.733471</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>vectorizer</td>\n","      <td>4</td>\n","      <td>1.000000</td>\n","      <td>0.433200</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>this</td>\n","      <td>2</td>\n","      <td>1.000000</td>\n","      <td>0.216600</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>for</td>\n","      <td>2</td>\n","      <td>1.000000</td>\n","      <td>0.216600</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>is</td>\n","      <td>2</td>\n","      <td>1.000000</td>\n","      <td>0.216600</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>count</td>\n","      <td>1</td>\n","      <td>1.693147</td>\n","      <td>0.183368</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>doesn</td>\n","      <td>1</td>\n","      <td>1.693147</td>\n","      <td>0.183368</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>key</td>\n","      <td>1</td>\n","      <td>1.287682</td>\n","      <td>0.139456</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>on</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>scoring</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>based</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>done</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>of</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>binary</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>presence</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>score</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>higher</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>tfidf</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>dummy</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>word</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      features  tf       idf  norm_tfidf\n","7    frequency   4  1.693147    0.733471\n","18  vectorizer   4  1.000000    0.433200\n","17        this   2  1.000000    0.216600\n","6          for   2  1.000000    0.216600\n","9           is   2  1.000000    0.216600\n","2        count   1  1.693147    0.183368\n","3        doesn   1  1.693147    0.183368\n","10         key   1  1.287682    0.139456\n","12          on   1  1.000000    0.108300\n","15     scoring   1  1.000000    0.108300\n","0        based   1  1.000000    0.108300\n","4         done   1  1.000000    0.108300\n","11          of   0  1.693147    0.000000\n","1       binary   0  1.693147    0.000000\n","13    presence   0  1.693147    0.000000\n","14       score   0  1.693147    0.000000\n","8       higher   0  1.693147    0.000000\n","16       tfidf   0  1.693147    0.000000\n","5        dummy   0  1.693147    0.000000\n","19        word   0  1.693147    0.000000"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# create dataframe for tfidf vectors for the first document\n","first_document_tfidf = tfidf_vectors[0].toarray().ravel()\n","feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n","df_tfidf = pd.DataFrame({'features': feature_names_tfidf,\n","                        'idf': term_idf, 'norm_tfidf': first_document_tfidf})\n","\n","# combine dataframes\n","\n","# Merge the tf and tf-idf dataframes on the 'features' column\n","df = pd.merge(left=df_tf, right=df_tfidf)\n","\n","# Sort the combined dataframe by the 'norm_tfidf' column in descending order\n","df.sort_values(by=[\"norm_tfidf\"], ascending=False, inplace=True)\n","\n","df\n"]},{"cell_type":"markdown","metadata":{"id":"Ft1Jj6GfxCEo"},"source":["**Observations from above results**\n","- words 'frequency' and 'vectorizer' occurs 4 times in the documsnt and hence term frequency is 4.\n","- Word 'vectorizer' occurs in every document and hence idf is 1 (log(1) + 1).\n","- norm_tfidf gives higher score to word 'frequency' than 'vectorizer'.\n","- norm_tfidf is not equal to idf * tf\n","\n","Let us know understand how norm_tfidf is calculated:"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T19:08:38.389005Z","iopub.status.busy":"2021-09-11T19:08:38.388843Z","iopub.status.idle":"2021-09-11T19:08:38.392671Z","shell.execute_reply":"2021-09-11T19:08:38.392237Z","shell.execute_reply.started":"2021-09-11T19:08:38.388992Z"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1706506328179,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"-k9PB58_xCEo","tags":[]},"outputs":[],"source":["# calculate tfidf (without any normalization)\n","df['tfidf'] = df.eval('tf*idf')\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2021-09-12T03:13:47.640165Z","iopub.status.busy":"2021-09-12T03:13:47.639897Z","iopub.status.idle":"2021-09-12T03:13:47.651782Z","shell.execute_reply":"2021-09-12T03:13:47.651488Z","shell.execute_reply.started":"2021-09-12T03:13:47.640137Z"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1706506329042,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"Betig4AjxCEp","tags":[]},"outputs":[],"source":["# calculate tfidf - normalized\n","df['sq_tfidf'] = df.eval('tfidf**2')\n","df['norm_tfidf_manually'] = df['tfidf']/np.sqrt(df['sq_tfidf'].sum())"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>features</th>\n","      <th>tf</th>\n","      <th>idf</th>\n","      <th>norm_tfidf</th>\n","      <th>tfidf</th>\n","      <th>sq_tfidf</th>\n","      <th>norm_tfidf_manually</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7</th>\n","      <td>frequency</td>\n","      <td>4</td>\n","      <td>1.693147</td>\n","      <td>0.733471</td>\n","      <td>6.772589</td>\n","      <td>45.867958</td>\n","      <td>0.733471</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>vectorizer</td>\n","      <td>4</td>\n","      <td>1.000000</td>\n","      <td>0.433200</td>\n","      <td>4.000000</td>\n","      <td>16.000000</td>\n","      <td>0.433200</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>this</td>\n","      <td>2</td>\n","      <td>1.000000</td>\n","      <td>0.216600</td>\n","      <td>2.000000</td>\n","      <td>4.000000</td>\n","      <td>0.216600</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>for</td>\n","      <td>2</td>\n","      <td>1.000000</td>\n","      <td>0.216600</td>\n","      <td>2.000000</td>\n","      <td>4.000000</td>\n","      <td>0.216600</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>is</td>\n","      <td>2</td>\n","      <td>1.000000</td>\n","      <td>0.216600</td>\n","      <td>2.000000</td>\n","      <td>4.000000</td>\n","      <td>0.216600</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>count</td>\n","      <td>1</td>\n","      <td>1.693147</td>\n","      <td>0.183368</td>\n","      <td>1.693147</td>\n","      <td>2.866747</td>\n","      <td>0.183368</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>doesn</td>\n","      <td>1</td>\n","      <td>1.693147</td>\n","      <td>0.183368</td>\n","      <td>1.693147</td>\n","      <td>2.866747</td>\n","      <td>0.183368</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>key</td>\n","      <td>1</td>\n","      <td>1.287682</td>\n","      <td>0.139456</td>\n","      <td>1.287682</td>\n","      <td>1.658125</td>\n","      <td>0.139456</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>on</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>scoring</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>based</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>done</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.108300</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>of</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>binary</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>presence</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>score</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>higher</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>tfidf</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>dummy</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>word</td>\n","      <td>0</td>\n","      <td>1.693147</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      features  tf       idf  norm_tfidf     tfidf   sq_tfidf  \\\n","7    frequency   4  1.693147    0.733471  6.772589  45.867958   \n","18  vectorizer   4  1.000000    0.433200  4.000000  16.000000   \n","17        this   2  1.000000    0.216600  2.000000   4.000000   \n","6          for   2  1.000000    0.216600  2.000000   4.000000   \n","9           is   2  1.000000    0.216600  2.000000   4.000000   \n","2        count   1  1.693147    0.183368  1.693147   2.866747   \n","3        doesn   1  1.693147    0.183368  1.693147   2.866747   \n","10         key   1  1.287682    0.139456  1.287682   1.658125   \n","12          on   1  1.000000    0.108300  1.000000   1.000000   \n","15     scoring   1  1.000000    0.108300  1.000000   1.000000   \n","0        based   1  1.000000    0.108300  1.000000   1.000000   \n","4         done   1  1.000000    0.108300  1.000000   1.000000   \n","11          of   0  1.693147    0.000000  0.000000   0.000000   \n","1       binary   0  1.693147    0.000000  0.000000   0.000000   \n","13    presence   0  1.693147    0.000000  0.000000   0.000000   \n","14       score   0  1.693147    0.000000  0.000000   0.000000   \n","8       higher   0  1.693147    0.000000  0.000000   0.000000   \n","16       tfidf   0  1.693147    0.000000  0.000000   0.000000   \n","5        dummy   0  1.693147    0.000000  0.000000   0.000000   \n","19        word   0  1.693147    0.000000  0.000000   0.000000   \n","\n","    norm_tfidf_manually  \n","7              0.733471  \n","18             0.433200  \n","17             0.216600  \n","6              0.216600  \n","9              0.216600  \n","2              0.183368  \n","3              0.183368  \n","10             0.139456  \n","12             0.108300  \n","15             0.108300  \n","0              0.108300  \n","4              0.108300  \n","11             0.000000  \n","1              0.000000  \n","13             0.000000  \n","14             0.000000  \n","8              0.000000  \n","16             0.000000  \n","5              0.000000  \n","19             0.000000  "]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"z1jKODj4iRNQ"},"source":["## <font color = 'pickle'>**Modifying Vocab**"]},{"cell_type":"markdown","metadata":{"id":"UeTUZZmJiccS"},"source":["### <font color = 'pickle'>**Case sensitive**"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:08:17.486661Z","iopub.status.busy":"2021-09-11T06:08:17.486532Z","iopub.status.idle":"2021-09-11T06:08:17.490721Z","shell.execute_reply":"2021-09-11T06:08:17.490336Z","shell.execute_reply.started":"2021-09-11T06:08:17.486649Z"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1706506331427,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"2omkpfjJnZ_L","outputId":"0de93a81-aba5-44a4-f03a-61700cc96f5a"},"outputs":[{"data":{"text/plain":["{'Count': 1,\n"," 'Vectorizer': 3,\n"," 'for': 8,\n"," 'this': 19,\n"," 'vectorizer': 20,\n"," 'scoring': 17,\n"," 'is': 11,\n"," 'done': 6,\n"," 'based': 4,\n"," 'on': 14,\n"," 'frequency': 9,\n"," 'For': 2,\n"," 'key': 12,\n"," 'doesn': 5,\n"," 'tfidf': 18,\n"," 'higher': 10,\n"," 'score': 16,\n"," 'Binary': 0,\n"," 'presence': 15,\n"," 'of': 13,\n"," 'word': 21,\n"," 'dummy': 7}"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# The lowercase argument is set to False to indicate that the text should\n","# not be converted to lowercase before tokenizing.\n","# The resulting vocab may have same word in upper and lower case\n","vectorizer = CountVectorizer(lowercase=False)\n","\n","# we can use fit_transform to use fit() and transform() in one step\n","vectors = vectorizer.fit_transform(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"75w3LP5li7VM"},"source":["### <font color = 'pickle'>**Filtering words based on frequency**"]},{"cell_type":"markdown","metadata":{"id":"RP_IKZG9jKvD"},"source":["The `max_df`, `min_df`, and `max_features` parameters in the `CountVectorizer`` class control the feature selection for the resulting term frequency (tf) vectors.\n","\n","- `max_df`: This parameter sets the maximum threshold for the frequency of a term in the document collection. If a term has a document frequency (i.e., the number of documents that contain the term) higher than max_df, it will be ignored. <font color = 'dodgerblue' >**This parameter is used to filter out stop words (corpus specific) that appear in too many documents.** </font>\n","\n","- min_df: This parameter sets the minimum threshold for the frequency of a term in the document collection. If a term has a document frequency lower than min_df, it will be ignored.  <font color = 'dodgerblue' >**This parameter is used to filter out rare words that appear in too few documents.**\n","\n","- max_features: This parameter sets the maximum number of features (i.e., the maximum number of unique terms) that should be included in the resulting tf vectors. If the number of unique terms in the document collection is larger than max_features, the terms with the highest tf values will be kept and the others will be ignored.  <font color = 'dodgerblue' >**This parameter is used to reduce the dimensionality of the resulting tf vectors, which can help reduce the computational cost of downstream processing.**\n","\n","By using the max_df, min_df, and max_features parameters, you can control the feature selection process and determine the most informative terms to include in the tf vectors."]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:08:40.000725Z","iopub.status.busy":"2021-09-11T06:08:40.000235Z","iopub.status.idle":"2021-09-11T06:08:40.007239Z","shell.execute_reply":"2021-09-11T06:08:40.006996Z","shell.execute_reply.started":"2021-09-11T06:08:40.000670Z"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1706506338734,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"YxzFCRGEMCkm","outputId":"fc903ed1-639d-4f54-a289-b69fa1cc1c4f"},"outputs":[{"data":{"text/plain":["{'vectorizer': 8,\n"," 'for': 2,\n"," 'this': 7,\n"," 'scoring': 6,\n"," 'is': 3,\n"," 'done': 1,\n"," 'based': 0,\n"," 'on': 5,\n"," 'key': 4}"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# remove rare words - remove words which appear in less than 2 documents\n","vectorizer = CountVectorizer(min_df=2)\n","vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:09:31.316083Z","iopub.status.busy":"2021-09-11T06:09:31.315553Z","iopub.status.idle":"2021-09-11T06:09:31.322432Z","shell.execute_reply":"2021-09-11T06:09:31.322174Z","shell.execute_reply.started":"2021-09-11T06:09:31.316023Z"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706506341162,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"bHDT20dLxCEq","outputId":"e80ad10b-1d72-4672-f16a-b748fff572d7"},"outputs":[{"data":{"text/plain":["{'count': 1,\n"," 'frequency': 4,\n"," 'key': 6,\n"," 'doesn': 2,\n"," 'tfidf': 10,\n"," 'higher': 5,\n"," 'score': 9,\n"," 'binary': 0,\n"," 'presence': 8,\n"," 'of': 7,\n"," 'word': 11,\n"," 'dummy': 3}"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# remove words which appear in more than 2 documents - remove corpus specific stop words\n","vectorizer = CountVectorizer(max_df=2)\n","vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:10:33.933882Z","iopub.status.busy":"2021-09-11T06:10:33.933354Z","iopub.status.idle":"2021-09-11T06:10:33.940572Z","shell.execute_reply":"2021-09-11T06:10:33.940289Z","shell.execute_reply.started":"2021-09-11T06:10:33.933825Z"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1706506343717,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"UW_UErB8xCEq","outputId":"f9024c26-8dc0-4d7b-d1f5-bc072ca86b95"},"outputs":[{"data":{"text/plain":["{'vectorizer': 4, 'for': 0, 'this': 3, 'is': 1, 'tfidf': 2}"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# retain most frequent words only - retain top n words based on term frequency across corpus\n","vectorizer = CountVectorizer(max_features=5)\n","vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"F6mnf0ZHxCEq"},"source":["### <font color = 'pickle'>**Stop Words**"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:14:28.793903Z","iopub.status.busy":"2021-09-11T06:14:28.793490Z","iopub.status.idle":"2021-09-11T06:14:28.800896Z","shell.execute_reply":"2021-09-11T06:14:28.800576Z","shell.execute_reply.started":"2021-09-11T06:14:28.793866Z"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1706506352061,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"XZ4dTc6YxCEq","outputId":"ed6dbe4a-5e35-4aef-cac3-24aa6e41057f"},"outputs":[{"data":{"text/plain":["{'vectorizer': 4, 'done': 1, 'based': 0, 'frequency': 2, 'tfidf': 3}"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["# We can also specify list of stopwords to countvectorizer to get the feature without stopwords\n","\n","# Import libraries\n","nltk_stop_words = nltk_stopwords.words('english')\n","\n","vectorizer = CountVectorizer(max_features=5, stop_words=nltk_stop_words)\n","vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"OWwFq44kxCEq"},"source":["### <font color = 'pickle'>**Custom Tokenizer and Preprocessor**"]},{"cell_type":"markdown","metadata":{"id":"MrVX1WEtxCEq"},"source":["#### <font color = 'pickle'>**nltk tokenizer**"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-12T03:16:09.705726Z","iopub.status.busy":"2021-09-12T03:16:09.705542Z","iopub.status.idle":"2021-09-12T03:16:09.711195Z","shell.execute_reply":"2021-09-12T03:16:09.710860Z","shell.execute_reply.started":"2021-09-12T03:16:09.705710Z"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1706506356666,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"XrmIi5seqljd","outputId":"fc7f5ff0-f133-4db5-ea77-0ef3977c22fc","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"data":{"text/plain":["{'count': 11,\n"," 'vectorizer': 28,\n"," '-': 4,\n"," 'for': 15,\n"," 'this': 27,\n"," ',': 3,\n"," 'scoring': 24,\n"," 'is': 18,\n"," 'done': 13,\n"," 'based': 9,\n"," 'on': 21,\n"," 'frequency': 16,\n"," '.': 5,\n"," 'key': 19,\n"," '@vectorizer': 8,\n"," '#frequency': 1,\n"," '@frequency': 7,\n"," 'doesn': 12,\n"," 'â€™': 30,\n"," 't': 25,\n"," 'tfidf': 26,\n"," 'higher': 17,\n"," 'score': 23,\n"," '#tfidf': 2,\n"," 'binary': 10,\n"," 'presence': 22,\n"," 'of': 20,\n"," 'word': 29,\n"," 'dummy': 14,\n"," '#dummy': 0,\n"," '@dummy': 6}"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# We can use custom tokenizer e.g. we can use nltk tweet tokenizer to get each tokens as feature\n","\n","# Create an instance of the TweetTokenizer class\n","tweet_tokenizer = TweetTokenizer()\n","\n","# Initialize the CountVectorizer with the custom tokenizer\n","# only works if analyzer = 'word'\n","vectorizer = CountVectorizer(\n","    analyzer='word', tokenizer=tweet_tokenizer.tokenize)\n","\n","vectorizer.fit_transform(Corpus)\n","vectorizer.vocabulary_"]},{"cell_type":"markdown","metadata":{"id":"nMzV-qHwxCEr"},"source":["#### <font color = 'pickle'>**spacy pre-processor and tokenizer**"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2021-09-12T03:16:12.204316Z","iopub.status.busy":"2021-09-12T03:16:12.204153Z","iopub.status.idle":"2021-09-12T03:16:12.207192Z","shell.execute_reply":"2021-09-12T03:16:12.206765Z","shell.execute_reply.started":"2021-09-12T03:16:12.204303Z"},"executionInfo":{"elapsed":45,"status":"ok","timestamp":1706506361628,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"nmyYzK9AVzXw","tags":[]},"outputs":[],"source":["def spacy_preprocessor(text):\n","\n","    # Create spacy object\n","    doc = nlp(text)\n","\n","    # remove punctuations and get a list of tokens\n","    filtered_text = [token.text for token in doc if not token.is_punct]\n","\n","    # join the processed tokens in to string\n","    return \" \".join(filtered_text)\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2021-09-12T03:16:12.863124Z","iopub.status.busy":"2021-09-12T03:16:12.862607Z","iopub.status.idle":"2021-09-12T03:16:12.872726Z","shell.execute_reply":"2021-09-12T03:16:12.870950Z","shell.execute_reply.started":"2021-09-12T03:16:12.863065Z"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1706506366026,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"YnFartqtWe6X","tags":[]},"outputs":[],"source":["# Spacy Tokenizer\n","def spacy_tokenizer(data):\n","    doc = nlp(data)\n","    return [token.text for token in doc]\n"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T20:36:02.024537Z","iopub.status.busy":"2021-09-11T20:36:02.024028Z","iopub.status.idle":"2021-09-11T20:36:02.043042Z","shell.execute_reply":"2021-09-11T20:36:02.042678Z","shell.execute_reply.started":"2021-09-11T20:36:02.024478Z"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1706506367193,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"S0GAbq-3WsT_","outputId":"95c958f5-9b17-4551-f8a2-28cc29889cc3","tags":[]},"outputs":[{"data":{"text/plain":["{'Count': 5,\n"," 'Vectorizer': 7,\n"," 'for': 12,\n"," 'this': 24,\n"," 'vectorizer': 25,\n"," 'scoring': 22,\n"," 'is': 15,\n"," 'done': 10,\n"," 'based': 8,\n"," 'on': 19,\n"," 'frequency': 13,\n"," 'For': 6,\n"," 'key': 16,\n"," '@vectorizer': 3,\n"," '@frequency': 2,\n"," 'does': 9,\n"," 'nâ€™t': 17,\n"," 'tfidf': 23,\n"," '  ': 0,\n"," 'higher': 14,\n"," 'score': 21,\n"," 'Binary': 4,\n"," 'presence': 20,\n"," 'of': 18,\n"," 'word': 26,\n"," 'dummy': 11,\n"," '@dummy': 1}"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["# custom preprocessor and spacy tokenizer\n","vectorizer = CountVectorizer(analyzer='word', preprocessor=spacy_preprocessor,\n","                             tokenizer=spacy_tokenizer, token_pattern=None)\n","vectors = vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"uSSOPlCRK38o"},"source":["#### <font color = 'pickle'>**custom preprocessor we created earlier**"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1706506376441,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"3xWMqbmm1PJl"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mInit signature:\u001b[0m\n","\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpacyPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mlemmatize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mremove_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mremove_punct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mremove_email\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mremove_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mremove_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mstemming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0madd_user_mention_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mremove_hashtag_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbasic_clean_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m     \n","A text preprocessor that utilizes spaCy for efficient and flexible NLP. Designed as a part of a scikit-learn\n","pipeline, it provides a wide range of text cleaning and preprocessing functionalities.\n","\n","Attributes:\n","    model (str): The spaCy language model to be used for tokenization and other NLP tasks.\n","    batch_size (int): The number of documents to process at once during spaCy's pipeline processing.\n","    lemmatize (bool): If True, lemmatize tokens.\n","    lower (bool): If True, convert all characters to lowercase.\n","    remove_stop (bool): If True, remove stopwords.\n","    remove_punct (bool): If True, remove punctuation.\n","    remove_email (bool): If True, remove email addresses.\n","    remove_url (bool): If True, remove URLs.\n","    remove_num (bool): If True, remove numbers.\n","    stemming (bool): If True, apply stemming to tokens (mutually exclusive with lemmatization).\n","    add_user_mention_prefix (bool): If True, add '@' as a separate token (useful for user mentions in social \n","        media data).\n","    remove_hashtag_prefix (bool): If True, do not separate '#' from the following text.\n","    basic_clean_only (bool): If True, perform only basic cleaning (HTML tags removal, line breaks, etc.) \n","        and ignore other preprocessing steps.\n","\n","Methods:\n","    basic_clean(text: str) -> str:\n","        Performs basic cleaning of the text such as removing HTML tags and excessive whitespace.\n","    \n","    spacy_preprocessor(texts: list) -> list:\n","        Processes a list of texts through the spaCy pipeline with specified preprocessing options.\n","    \n","    fit(X, y=None) -> 'SpacyPreprocessor':\n","        Fits the preprocessor to the data. This is a dummy method for scikit-learn compatibility and does not \n","        change the state of the object.\n","    \n","    transform(X, y=None) -> list:\n","        Transforms the provided data using the defined preprocessing pipeline. Performs basic cleaning, \n","        and if `basic_clean_only` is False, it applies advanced spaCy preprocessing steps.\n","\n","Raises:\n","    ValueError: If both 'lemmatize' and 'stemming' are set to True.\n","    ValueError: If 'basic_clean_only' is True but other processing options are also set to True.\n","    TypeError: If the input X is not a list or a numpy array.\n","\u001b[0;31mSource:\u001b[0m        \n","\u001b[0;32mclass\u001b[0m \u001b[0mSpacyPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n","\u001b[0;34m    A text preprocessor that utilizes spaCy for efficient and flexible NLP. Designed as a part of a scikit-learn\u001b[0m\n","\u001b[0;34m    pipeline, it provides a wide range of text cleaning and preprocessing functionalities.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    Attributes:\u001b[0m\n","\u001b[0;34m        model (str): The spaCy language model to be used for tokenization and other NLP tasks.\u001b[0m\n","\u001b[0;34m        batch_size (int): The number of documents to process at once during spaCy's pipeline processing.\u001b[0m\n","\u001b[0;34m        lemmatize (bool): If True, lemmatize tokens.\u001b[0m\n","\u001b[0;34m        lower (bool): If True, convert all characters to lowercase.\u001b[0m\n","\u001b[0;34m        remove_stop (bool): If True, remove stopwords.\u001b[0m\n","\u001b[0;34m        remove_punct (bool): If True, remove punctuation.\u001b[0m\n","\u001b[0;34m        remove_email (bool): If True, remove email addresses.\u001b[0m\n","\u001b[0;34m        remove_url (bool): If True, remove URLs.\u001b[0m\n","\u001b[0;34m        remove_num (bool): If True, remove numbers.\u001b[0m\n","\u001b[0;34m        stemming (bool): If True, apply stemming to tokens (mutually exclusive with lemmatization).\u001b[0m\n","\u001b[0;34m        add_user_mention_prefix (bool): If True, add '@' as a separate token (useful for user mentions in social \u001b[0m\n","\u001b[0;34m            media data).\u001b[0m\n","\u001b[0;34m        remove_hashtag_prefix (bool): If True, do not separate '#' from the following text.\u001b[0m\n","\u001b[0;34m        basic_clean_only (bool): If True, perform only basic cleaning (HTML tags removal, line breaks, etc.) \u001b[0m\n","\u001b[0;34m            and ignore other preprocessing steps.\u001b[0m\n","\u001b[0;34m\u001b[0m\n","\u001b[0;34m    Methods:\u001b[0m\n","\u001b[0;34m        basic_clean(text: str) -> str:\u001b[0m\n","\u001b[0;34m            Performs basic cleaning of the text such as removing HTML tags and excessive whitespace.\u001b[0m\n","\u001b[0;34m        \u001b[0m\n","\u001b[0;34m        spacy_preprocessor(texts: list) -> list:\u001b[0m\n","\u001b[0;34m            Processes a list of texts through the spaCy pipeline with specified preprocessing options.\u001b[0m\n","\u001b[0;34m        \u001b[0m\n","\u001b[0;34m        fit(X, y=None) -> 'SpacyPreprocessor':\u001b[0m\n","\u001b[0;34m            Fits the preprocessor to the data. This is a dummy method for scikit-learn compatibility and does not \u001b[0m\n","\u001b[0;34m            change the state of the object.\u001b[0m\n","\u001b[0;34m        \u001b[0m\n","\u001b[0;34m        transform(X, y=None) -> list:\u001b[0m\n","\u001b[0;34m            Transforms the provided data using the defined preprocessing pipeline. Performs basic cleaning, \u001b[0m\n","\u001b[0;34m            and if `basic_clean_only` is False, it applies advanced spaCy preprocessing steps.\u001b[0m\n","\u001b[0;34m    \u001b[0m\n","\u001b[0;34m    Raises:\u001b[0m\n","\u001b[0;34m        ValueError: If both 'lemmatize' and 'stemming' are set to True.\u001b[0m\n","\u001b[0;34m        ValueError: If 'basic_clean_only' is True but other processing options are also set to True.\u001b[0m\n","\u001b[0;34m        TypeError: If the input X is not a list or a numpy array.\u001b[0m\n","\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0mremove_punct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_email\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0madd_user_mention_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_hashtag_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasic_clean_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stop\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punct\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_num\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_url\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_email\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_email\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_user_mention_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_user_mention_prefix\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_hashtag_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_hashtag_prefix\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_clean_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasic_clean_only\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlemmatize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstemming\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only one of 'lemmatize' and 'stemming' can be True.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# Validate basic_clean_only option\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_clean_only\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlemmatize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlower\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mremove_stop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mremove_punct\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mremove_num\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstemming\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                                      \u001b[0madd_user_mention_prefix\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mremove_hashtag_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"If 'basic_clean_only' is set to True, other processing options must be set to False.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# Assign lemmatize and stemming\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemming\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbasic_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[\\n\\r]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mspacy_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mfinal_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;31m# Disable unnecessary pipelines in spaCy model\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;31m# Disable parser and named entity recognition\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mdisabled_pipes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'parser'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;31m# Disable tagger, parser, attribute ruler, lemmatizer and named entity recognition\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mdisabled_pipes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'tok2vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tagger'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parser'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attribute_ruler'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lemmatizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_pipes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisabled_pipes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m          \u001b[0;31m# Modify tokenizer behavior based on user_mention_prefix and hashtag_prefix settings\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m          \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_user_mention_prefix\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_hashtag_prefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m              \u001b[0mprefixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m              \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_user_mention_prefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0mprefixes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'@'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Treat '@' as a separate token\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m              \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_hashtag_prefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0mprefixes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'#'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Don't separate '#' from the following text\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m              \u001b[0mprefix_regex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_prefix_regex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m              \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m          \u001b[0;31m# Process text data in parallel using spaCy's nlp.pipe()\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m              \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m              \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0;31m# Check if token should be removed based on specified filters\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_stop\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                      \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_punct\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_punct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                      \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_num\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlike_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                      \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_url\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlike_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                      \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_email\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlike_email\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                      \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0;31m# Append the token's text, lemma, or stemmed form to the filtered_tokens list\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                      \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemming\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                      \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                      \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m              \u001b[0;31m# Join the tokens and apply lowercasing if specified\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m              \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m              \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                  \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m              \u001b[0mfinal_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_result\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Expected list or numpy array, got {type(X)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mx_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;31m# Check if only basic cleaning is required\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_clean_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m                \u001b[0;32mreturn\u001b[0m \u001b[0mx_clean\u001b[0m  \u001b[0;31m# Return the list of basic-cleaned texts\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mx_clean_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mx_clean_final\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'An exception occurred: {repr(error)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFile:\u001b[0m           ~/Library/CloudStorage/GoogleDrive-harikrish0607@gmail.com/My Drive/Colab_Notebooks/BUAN_6342_Applied_Natural_Language_Processing/0_Custom_files/CustomPreprocessorSpacy.py\n","\u001b[0;31mType:\u001b[0m           type\n","\u001b[0;31mSubclasses:\u001b[0m     "]}],"source":["cp.SpacyPreprocessor??"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1706506380630,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"EcX3HWLFF2qU"},"outputs":[],"source":["custom_preprocessor = cp.SpacyPreprocessor('en_core_web_sm')\n"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1706506382093,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"BKUl2hjTI1Ig"},"outputs":[],"source":["def spacy_preprocessor(text):\n","    filtered_text = custom_preprocessor.transform([text])\n","    return \" \".join(filtered_text)\n"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1268,"status":"ok","timestamp":1706506404600,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"iy5XeSb6GHwU","outputId":"7ad37596-d530-420d-b9ac-4a83749c3e19"},"outputs":[{"data":{"text/plain":["{'count': 2,\n"," 'vectorizer': 11,\n"," 'scoring': 9,\n"," 'base': 0,\n"," 'frequency': 4,\n"," 'key': 6,\n"," 'tfidf': 10,\n"," 'high': 5,\n"," 'score': 8,\n"," 'binary': 1,\n"," 'presence': 7,\n"," 'word': 12,\n"," 'dummy': 3}"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["# custom preprocessor and spacy tokenizer\n","vectorizer = CountVectorizer(analyzer='word', preprocessor=spacy_preprocessor,\n","                            token_pattern=r\"[\\S]+\")\n","vectors = vectorizer.fit(Corpus)\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"STiVvP5qxCEr"},"source":["#### <font color = 'pickle'>**token patterns with regular expressions**"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:32:57.889450Z","iopub.status.busy":"2021-09-11T06:32:57.888935Z","iopub.status.idle":"2021-09-11T06:32:57.893152Z","shell.execute_reply":"2021-09-11T06:32:57.892901Z","shell.execute_reply.started":"2021-09-11T06:32:57.889434Z"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1706506407129,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"2xEae96cC5e3","outputId":"f30c3559-5c7e-4314-92e3-e36fdefb90ec"},"outputs":[{"data":{"text/plain":["{'count': 9,\n"," 'vectorizer': 28,\n"," '-': 3,\n"," 'for': 13,\n"," 'this': 27,\n"," 'vectorizer,': 29,\n"," 'scoring': 24,\n"," 'is': 17,\n"," 'done': 11,\n"," 'based': 7,\n"," 'on': 21,\n"," 'frequency.': 15,\n"," 'frequency': 14,\n"," 'key.': 19,\n"," '@vectorizer': 6,\n"," '#frequency': 1,\n"," '@frequency,': 5,\n"," 'doesnâ€™t': 10,\n"," 'tfidf': 25,\n"," 'tfidf,': 26,\n"," 'higher': 16,\n"," 'score': 23,\n"," '#tfidf': 2,\n"," 'binary': 8,\n"," 'presence': 22,\n"," 'of': 20,\n"," 'word.': 30,\n"," 'dummy': 12,\n"," 'key': 18,\n"," '#dummy': 0,\n"," '@dummy': 4}"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# We can pass regex to the argument token_pattern to get required pattern\n","# whitespace tokenizer\n","# This can be very useful if we have allready cleaned the text\n","vectorizer = CountVectorizer(analyzer='word', token_pattern=r\"[\\S]+\")\n","\n","# Assign the encoded(transformed) vectors to a variable\n","vectors = vectorizer.fit_transform(Corpus)\n","\n","vectorizer.vocabulary_\n"]},{"cell_type":"markdown","metadata":{"id":"haBuVWSyxCEs"},"source":["### <font color = 'pickle'>**ngrams**</font>\n","\n","- Till now our features consists of single token. However, in some cases we may want to use sequence of tokens as features\n","- Consider the following corpus\n"," 1. This item is good\n"," 2. This item is not good\n","- Now  both the documents will have feature 'good' and 'not' will be an additional feature in document 2.\n","- For applications like sentiment analysis - it might be a good idea to consider 'not good' as a single token.\n","\n","- We can use ngram_range(min_n, max_n) in CountVectorizer to create features that consists of sequence of words.\n","\n","- if we specify min_n = 2 and max_n = 3, we will get bigrams and trigrams as features."]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T06:47:29.553885Z","iopub.status.busy":"2021-09-11T06:47:29.553741Z","iopub.status.idle":"2021-09-11T06:47:29.559538Z","shell.execute_reply":"2021-09-11T06:47:29.559122Z","shell.execute_reply.started":"2021-09-11T06:47:29.553873Z"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706503595430,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"Ix4jKILgDLxa","outputId":"fc7f4b34-4600-4269-8786-b1511d8d5178","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Features for text 1\n","\n","is not\n","item is\n","not good\n","this item\n","\n","Features for text 2\n","\n","is terribly\n","item is\n","terribly good\n","this item\n"]}],"source":["min_n = 2\n","max_n = 2\n","\n","# only works if analyzer = 'word'\n","vectorizer1 = CountVectorizer(analyzer='word', ngram_range=(min_n, max_n))\n","vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(min_n, max_n))\n","\n","text1 = [\"This item is not good\"]\n","text2 = [\"This item is terribly good\"]\n","\n","# Fit the vectorizer to text\n","vectorizer1.fit_transform(text1)\n","vectorizer2.fit_transform(text2)\n","\n","features1 = vectorizer1.get_feature_names_out()\n","features2 = vectorizer2.get_feature_names_out()\n","\n","print('Features for text 1\\n')\n","for feature in features1:\n","    print(feature)\n","\n","print(f'\\nFeatures for text 2\\n')\n","for feature in features2:\n","    print(feature)"]},{"cell_type":"markdown","metadata":{"id":"7eaVPcHeRXRl"},"source":["## <font color = 'pickle'>**Example : IMDB Data set**"]},{"cell_type":"markdown","metadata":{"id":"wr_e08FxxCEs"},"source":["### <font color = 'pickle'>**Import Data**"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["'/Users/harikrishnadev/Library/CloudStorage/GoogleDrive-harikrish0607@gmail.com/My Drive/Colab_Notebooks/BUAN_6342_Applied_Natural_Language_Processing/'"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["basepath"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T07:45:50.456908Z","iopub.status.busy":"2021-09-11T07:45:50.456747Z","iopub.status.idle":"2021-09-11T07:45:50.459467Z","shell.execute_reply":"2021-09-11T07:45:50.459100Z","shell.execute_reply.started":"2021-09-11T07:45:50.456895Z"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1706503607291,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"TnakQosgOsMY","tags":[]},"outputs":[],"source":["# Use train.csv of IMDB movie review data (we downloaded this in the last lecture)\n","base_folder = Path(basepath)\n","data_folder = base_folder / '0_Data_Folder'\n","train_data = data_folder / 'train.csv'\n","test_data = data_folder /'test.csv'\n"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"execution":{"iopub.execute_input":"2021-09-11T07:53:35.249495Z","iopub.status.busy":"2021-09-11T07:53:35.249349Z","iopub.status.idle":"2021-09-11T07:53:35.584630Z","shell.execute_reply":"2021-09-11T07:53:35.584212Z","shell.execute_reply.started":"2021-09-11T07:53:35.249483Z"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1706503617189,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"2CgPhuOixCEs","outputId":"5f3f0b4c-f674-4bf1-f3b9-8ea84e9d7d51","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of Training data set is : (25000, 2)\n","Shape of Test data set is : (25000, 2)\n","\n","Top five rows of Training data set:\n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Reviews</th>\n","      <th>Labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ever wanted to know just how much Hollywood co...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The movie itself was ok for the kids. But I go...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>You could stage a version of Charles Dickens' ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>this was a fantastic episode. i saw a clip fro...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>and laugh out loud funny in many scenes.&lt;br /&gt;...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             Reviews  Labels\n","0  Ever wanted to know just how much Hollywood co...       1\n","1  The movie itself was ok for the kids. But I go...       1\n","2  You could stage a version of Charles Dickens' ...       1\n","3  this was a fantastic episode. i saw a clip fro...       1\n","4  and laugh out loud funny in many scenes.<br />...       1"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["# Reading data\n","train_df = pd.read_csv(train_data, index_col=0)\n","test_df = pd.read_csv(test_data, index_col=0)\n","print(f'Shape of Training data set is : {train_df.shape}')\n","print(f'Shape of Test data set is : {test_df.shape}')\n","print(f'\\nTop five rows of Training data set:\\n')\n","train_df.head()\n"]},{"cell_type":"markdown","metadata":{"id":"FDh2M0G9xCEs"},"source":["### <font color = 'pickle'>**Generating Vocab**</font>\n","- <font color = 'indianred'>**Vocab should be created only based on training dataset**</font>\n","- We will generate vocab using CountVectorizer\n","- <font color = 'indianred'>**Use fit_transform() on Training data set**.\n","- **Use only transform() on Test dataset**. This make sures that we generate vocab only based on training dataset."]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":161},"execution":{"iopub.execute_input":"2021-09-11T07:46:54.988708Z","iopub.status.busy":"2021-09-11T07:46:54.988551Z","iopub.status.idle":"2021-09-11T07:46:57.356248Z","shell.execute_reply":"2021-09-11T07:46:57.355769Z","shell.execute_reply.started":"2021-09-11T07:46:54.988695Z"},"executionInfo":{"elapsed":2474,"status":"ok","timestamp":1706506470944,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"rudMW4B_UGVE","outputId":"2f2bd06b-690a-4bbd-cfc8-df72492379b2"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-3 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-3 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-3 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-3 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-3 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-3 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-3 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-3 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-3 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-3 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-3 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-3 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-3 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-3 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-3 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-3 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"â–¸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-3 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-3 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-3 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"â–¾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-3 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-3 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-3 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-3 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-3 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-3 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-3 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-3 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-3 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-3 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n","                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n","                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n","                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n","                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n","                            &#x27;itself&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n","                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n","                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n","                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n","                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n","                            &#x27;itself&#x27;, ...])</pre></div> </div></div></div></div>"],"text/plain":["CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n","                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n","                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n","                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n","                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n","                            'itself', ...])"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize vectorizer\n","nltk_stop_words = nltk_stopwords.words('english')\n","bag_of_word = CountVectorizer(stop_words=nltk_stop_words)\n","\n","# Fit on training data\n","bag_of_word.fit(train_df['Reviews'].values)\n"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T07:47:13.492919Z","iopub.status.busy":"2021-09-11T07:47:13.492761Z","iopub.status.idle":"2021-09-11T07:47:13.527237Z","shell.execute_reply":"2021-09-11T07:47:13.526788Z","shell.execute_reply.started":"2021-09-11T07:47:13.492905Z"},"executionInfo":{"elapsed":65,"status":"ok","timestamp":1706506475057,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"6DsUyK2LVCg4","tags":[]},"outputs":[{"data":{"text/plain":["array(['00', '000', '0000000000001', ..., 'Ã¸stbye', 'Ã¼ber', 'Ã¼vegtigris'],\n","      dtype=object)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["# get feature names\n","features = bag_of_word.get_feature_names_out()\n","features"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T07:47:20.015894Z","iopub.status.busy":"2021-09-11T07:47:20.015734Z","iopub.status.idle":"2021-09-11T07:47:20.019068Z","shell.execute_reply":"2021-09-11T07:47:20.018602Z","shell.execute_reply.started":"2021-09-11T07:47:20.015881Z"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1706506475991,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"yIDLMOzoxCEt","outputId":"39e5c394-231a-4c31-e526-557f9ef896eb"},"outputs":[{"data":{"text/plain":["74704"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["# check the legth of the vocab\n","len(features)\n"]},{"cell_type":"markdown","metadata":{"id":"MxjTROQkxCEt"},"source":["### <font color = 'pickle'>**Create vectors for reviews**"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T08:00:01.403847Z","iopub.status.busy":"2021-09-11T08:00:01.403637Z","iopub.status.idle":"2021-09-11T08:00:05.617042Z","shell.execute_reply":"2021-09-11T08:00:05.616588Z","shell.execute_reply.started":"2021-09-11T08:00:01.403825Z"},"executionInfo":{"elapsed":4517,"status":"ok","timestamp":1706506482643,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"sUvgnrjZVJYn"},"outputs":[],"source":["# Transform the training and test dataset\n","bow_vector_train = bag_of_word.transform(train_df['Reviews'].values)\n","bow_vector_test = bag_of_word.transform(test_df['Reviews'].values)\n"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T08:00:08.442306Z","iopub.status.busy":"2021-09-11T08:00:08.442071Z","iopub.status.idle":"2021-09-11T08:00:08.446901Z","shell.execute_reply":"2021-09-11T08:00:08.446266Z","shell.execute_reply.started":"2021-09-11T08:00:08.442282Z"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1706506483975,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"6xk4M3NPxCEt","outputId":"72d83048-915c-44c1-88cc-a8f54bd28fcb"},"outputs":[{"data":{"text/plain":["<25000x74704 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 2479678 stored elements in Compressed Sparse Row format>"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["# Shape of the matrix for train dataset\n","bow_vector_train\n"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-09-11T08:00:56.304151Z","iopub.status.busy":"2021-09-11T08:00:56.303619Z","iopub.status.idle":"2021-09-11T08:00:56.314758Z","shell.execute_reply":"2021-09-11T08:00:56.312719Z","shell.execute_reply.started":"2021-09-11T08:00:56.304091Z"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706506486323,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"_LALNShqxCEt","outputId":"71ad1cfb-fda2-4807-de03-7820d57b5fbb"},"outputs":[{"data":{"text/plain":["<25000x74704 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 2385031 stored elements in Compressed Sparse Row format>"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["# Shape of the matrix for test dataset\n","bow_vector_test\n"]},{"cell_type":"markdown","metadata":{"id":"uaUcb2LBXBhw"},"source":["### <font color = 'pickle'>**Limit vocab using max_features**\n","We got 25k rows with 78k+ features, but what if we want only top 5k features.\n","We can do this by providing max_features parameter."]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":2301,"status":"ok","timestamp":1706506492872,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"zR0AfavvHeQn","outputId":"4d30eeef-d6b1-48a2-fec7-706a7cef2ccd"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n","  warnings.warn(\n"]},{"data":{"text/html":["<style>#sk-container-id-4 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-4 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-4 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-4 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-4 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-4 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-4 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-4 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-4 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-4 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-4 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-4 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-4 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-4 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-4 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-4 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"â–¸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-4 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-4 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-4 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"â–¾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-4 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-4 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-4 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-4 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-4 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-4 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-4 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-4 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-4 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-4 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_features=5000,\n","                stop_words=[&#x27;namely&#x27;, &#x27;now&#x27;, &#x27;empty&#x27;, &#x27;while&#x27;, &#x27;nâ€™t&#x27;, &#x27;nowhere&#x27;,\n","                            &#x27;hers&#x27;, &#x27;we&#x27;, &quot;&#x27;s&quot;, &#x27;against&#x27;, &#x27;forty&#x27;, &#x27;used&#x27;,\n","                            &#x27;among&#x27;, &#x27;he&#x27;, &#x27;otherwise&#x27;, &#x27;yourselves&#x27;, &#x27;except&#x27;,\n","                            &#x27;them&#x27;, &#x27;should&#x27;, &#x27;across&#x27;, &#x27;full&#x27;, &#x27;because&#x27;, &#x27;ca&#x27;,\n","                            &#x27;further&#x27;, &#x27;ever&#x27;, &#x27;under&#x27;, &#x27;became&#x27;, &#x27;by&#x27;,\n","                            &#x27;thence&#x27;, &#x27;which&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer(max_features=5000,\n","                stop_words=[&#x27;namely&#x27;, &#x27;now&#x27;, &#x27;empty&#x27;, &#x27;while&#x27;, &#x27;nâ€™t&#x27;, &#x27;nowhere&#x27;,\n","                            &#x27;hers&#x27;, &#x27;we&#x27;, &quot;&#x27;s&quot;, &#x27;against&#x27;, &#x27;forty&#x27;, &#x27;used&#x27;,\n","                            &#x27;among&#x27;, &#x27;he&#x27;, &#x27;otherwise&#x27;, &#x27;yourselves&#x27;, &#x27;except&#x27;,\n","                            &#x27;them&#x27;, &#x27;should&#x27;, &#x27;across&#x27;, &#x27;full&#x27;, &#x27;because&#x27;, &#x27;ca&#x27;,\n","                            &#x27;further&#x27;, &#x27;ever&#x27;, &#x27;under&#x27;, &#x27;became&#x27;, &#x27;by&#x27;,\n","                            &#x27;thence&#x27;, &#x27;which&#x27;, ...])</pre></div> </div></div></div></div>"],"text/plain":["CountVectorizer(max_features=5000,\n","                stop_words=['namely', 'now', 'empty', 'while', 'nâ€™t', 'nowhere',\n","                            'hers', 'we', \"'s\", 'against', 'forty', 'used',\n","                            'among', 'he', 'otherwise', 'yourselves', 'except',\n","                            'them', 'should', 'across', 'full', 'because', 'ca',\n","                            'further', 'ever', 'under', 'became', 'by',\n","                            'thence', 'which', ...])"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["# Limit Vocab size using Max features\n","spacy_stop_words = nlp.Defaults.stop_words\n","bag_of_word = CountVectorizer(\n","    max_features=5000, stop_words=list(spacy_stop_words))  # Max features\n","\n","# Fit on training data\n","bag_of_word.fit(train_df['Reviews'].values)\n"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2021-09-11T08:02:57.092677Z","iopub.status.busy":"2021-09-11T08:02:57.092551Z","iopub.status.idle":"2021-09-11T08:03:01.788423Z","shell.execute_reply":"2021-09-11T08:03:01.787845Z","shell.execute_reply.started":"2021-09-11T08:02:57.092664Z"},"executionInfo":{"elapsed":4524,"status":"ok","timestamp":1706506501281,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"rEq04frRXq_i"},"outputs":[],"source":["# Transform the training and test dataset\n","bow_vector_train = bag_of_word.transform(train_df['Reviews'].values)\n","bow_vector_test = bag_of_word.transform(train_df['Reviews'].values)\n"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.execute_input":"2021-09-11T08:03:51.691500Z","iopub.status.busy":"2021-09-11T08:03:51.691259Z","iopub.status.idle":"2021-09-11T08:03:51.887702Z","shell.execute_reply":"2021-09-11T08:03:51.887320Z","shell.execute_reply.started":"2021-09-11T08:03:51.691476Z"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1706506502539,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"6EZxdHGaY2Ha","outputId":"3eb6d350-723e-4d3a-e9cf-a52222f7ee48"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>00</th>\n","      <th>000</th>\n","      <th>10</th>\n","      <th>100</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>13th</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>...</th>\n","      <th>yesterday</th>\n","      <th>york</th>\n","      <th>young</th>\n","      <th>younger</th>\n","      <th>youth</th>\n","      <th>zero</th>\n","      <th>zizek</th>\n","      <th>zombie</th>\n","      <th>zombies</th>\n","      <th>zone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24996</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24997</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24998</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24999</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>25000 rows Ã— 5000 columns</p>\n","</div>"],"text/plain":["       00  000  10  100  11  12  13  13th  14  15  ...  yesterday  york  \\\n","0       0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","1       0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","2       0    0   0    0   0   0   0     0   0   0  ...          0     1   \n","3       0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","4       0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","...    ..  ...  ..  ...  ..  ..  ..   ...  ..  ..  ...        ...   ...   \n","24995   0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","24996   0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","24997   0    0   0    0   0   0   0     0   0   1  ...          0     0   \n","24998   0    0   1    0   0   0   0     0   0   0  ...          0     0   \n","24999   0    0   0    0   0   0   0     0   0   0  ...          0     0   \n","\n","       young  younger  youth  zero  zizek  zombie  zombies  zone  \n","0          1        0      0     0      0       0        0     0  \n","1          0        0      0     0      0       0        0     0  \n","2          0        0      0     0      0       0        0     0  \n","3          0        0      0     0      0       0        0     0  \n","4          0        0      0     0      0       0        0     0  \n","...      ...      ...    ...   ...    ...     ...      ...   ...  \n","24995      0        0      0     0      0       0        0     0  \n","24996      0        0      0     0      0       0        0     0  \n","24997      0        1      0     0      0       0        0     0  \n","24998      0        0      0     0      0       0        0     0  \n","24999      0        0      0     0      0       0        0     0  \n","\n","[25000 rows x 5000 columns]"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["# Document representation\n","vocab = bag_of_word.get_feature_names_out()\n","pd.DataFrame(bow_vector_train.toarray(), columns=vocab)\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
