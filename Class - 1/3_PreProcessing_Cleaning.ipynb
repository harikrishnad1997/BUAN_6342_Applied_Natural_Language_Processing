{"cells":[{"cell_type":"markdown","metadata":{"id":"JmvijURlnQOq"},"source":["# <font color = 'dodgerblue'>**PreProcessing_Feature_Emgineering_IMDB**\n","In this notebook, we will do basic_proprpcessing followed by Featuyre Engineering.\n","\n","In preprocessing, we will do following:\n","\n","(1) Remove HTML Tags\n","(2) Remove emails\n","(3) Remove URLs\n","\n","After Preprocessing, we will add following faetures:\n","\n","  1. number of words\n","  2. number of characters\n","  3. number of characters without space\n","  4. average word length\n","  5. number of numbers (tokens that are numbers)\n","  6. number of sentences\n","  6. number of nouns or propernouns\n","  7. number of aux\n","  8. number of verbs\n","  9. number of adjectives\n","  10. number of ner (entiites)\n","\n","Finally, we will craete classes which we can reuse for (1) general preprocessing and (2) manual faeture extraction. We will make sure our classes are compatible with sklearn pipelines."]},{"cell_type":"markdown","metadata":{"id":"dsuaOxU-tG5D"},"source":["# <font color = 'dodgerblue'>**Install/Import Libraries**"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T09:04:46.983241Z","iopub.status.busy":"2023-01-30T09:04:46.982952Z","iopub.status.idle":"2023-01-30T09:04:46.992689Z","shell.execute_reply":"2023-01-30T09:04:46.991818Z","shell.execute_reply.started":"2023-01-30T09:04:46.983163Z"},"id":"SYkzEkPRwVbC","tags":[],"executionInfo":{"status":"ok","timestamp":1705900714684,"user_tz":360,"elapsed":11670,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"526e0e5f-0503-4959-befc-3fb0b6173348"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m980.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# install spacy\n","if 'google.colab' in str(get_ipython()):\n","    !pip install -U spacy -qq\n","\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T09:04:47.316892Z","iopub.status.busy":"2023-01-30T09:04:47.316658Z","iopub.status.idle":"2023-01-30T09:04:49.307720Z","shell.execute_reply":"2023-01-30T09:04:49.307364Z","shell.execute_reply.started":"2023-01-30T09:04:47.316866Z"},"id":"kV1KnxhOMK-0","tags":[],"executionInfo":{"status":"ok","timestamp":1705901626523,"user_tz":360,"elapsed":3,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# Import the Path module from the pathlib library\n","from pathlib import Path\n","\n","# Import the pandas library for working with data frames\n","import pandas as pd\n","import numpy as np\n","\n","# Import the spacy library for natural language processing\n","import spacy\n","\n","# Import the List type from the typing module to use in function annotations\n","from typing import List\n","\n","from bs4 import BeautifulSoup\n","import re\n","\n","from pprint import pprint"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"execution":{"iopub.execute_input":"2023-01-30T09:04:49.308462Z","iopub.status.busy":"2023-01-30T09:04:49.308376Z","iopub.status.idle":"2023-01-30T09:04:49.313574Z","shell.execute_reply":"2023-01-30T09:04:49.313316Z","shell.execute_reply.started":"2023-01-30T09:04:49.308450Z"},"executionInfo":{"elapsed":122,"status":"ok","timestamp":1705900739979,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"fRNfn9z_wXzL","outputId":"57d01153-c405-4d86-bfcb-bc8546932773","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.7.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["# check spacy version\n","spacy.__version__\n"]},{"cell_type":"markdown","metadata":{"id":"D3sRlk1-tG5N"},"source":["# <font color = 'dodgerblue'>**Data**"]},{"cell_type":"code","source":["text = [\"\"\"New version of operation system is iOS 11. It is better than iOS 9.\n","The new version of iPhone X seems cool.\"\"\", \"\"\" <p> The video of iphone x released. I liked iOS 9 but I like iOS 11 more.\n","You may not like my like @Tech_Guru #Iphone #IOS harpreet@utdallas.edu  https://jindal.utdallas.edu/\"\"\",\n","        \"\"\"</p><p>The concept of regular expressions began in the 1950s, when the American mathematician <a href=\"/wiki/Stephen_Cole_Kleene\" title=\"Stephen Cole Kleene\">Stephen Cole Kleene</a> formalized the description of a <i><a href=\"/wiki/Regular_language\" title=\"Regular language\">regular language</a></i>. They came into common use with <a href=\"/wiki/Unix\" title=\"Unix\">Unix</a> text-processing utilities. Different <a href=\"/wiki/Syntax_(programming_languages)\" title=\"Syntax (programming languages)\">syntaxes</a> for writing regular expressions have existed since the 1980s, one being the <a href=\"/wiki/POSIX\" title=\"POSIX\">POSIX</a> standard and another, widely used, being the <a href=\"/wiki/Perl\" title=\"Perl\">Perl</a> syntax.\n","</p><p>Regular expressions are used in <a href=\"/wiki/Search_engine\" title=\"Search engine\">search engines</a>, search and replace dialogs of <a href=\"/wiki/Word_processor\" title=\"Word processor\">word processors</a> and <a href=\"/wiki/Text_editor\" title=\"Text editor\">text editors</a>, in <a href=\"/wiki/Text_processing\" title=\"Text processing\">text processing</a> utilities such as <a href=\"/wiki/Sed\" title=\"Sed\">sed</a> and <a href=\"/wiki/AWK\" title=\"AWK\">AWK</a> and in <a href=\"/wiki/Lexical_analysis\" title=\"Lexical analysis\">lexical analysis</a>. Many <a href=\"/wiki/Programming_language\" title=\"Programming language\">programming languages</a> provide regex capabilities either built-in or via <a href=\"/wiki/Library_(computing)\" title=\"Library (computing)\">libraries</a>, as it has uses in many situations.\n","</p> \"\"\"]\n","df = pd.DataFrame(text, columns=['Reviews'])"],"metadata":{"id":"EmypysdLqKMo","executionInfo":{"status":"ok","timestamp":1705902424496,"user_tz":360,"elapsed":227,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"Io_Y6445tT-K","executionInfo":{"status":"ok","timestamp":1705902430662,"user_tz":360,"elapsed":146,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"c38f4f1d-930d-4adc-be26-297bd246ee48"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             Reviews\n","0  New version of operation system is iOS 11. It ...\n","1   <p> The video of iphone x released. I liked i...\n","2  </p><p>The concept of regular expressions bega..."],"text/html":["\n","  <div id=\"df-284fdb0c-0f80-4ca7-af93-9823247e0e33\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Reviews</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>New version of operation system is iOS 11. It ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;p&gt; The video of iphone x released. I liked i...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>&lt;/p&gt;&lt;p&gt;The concept of regular expressions bega...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-284fdb0c-0f80-4ca7-af93-9823247e0e33')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-284fdb0c-0f80-4ca7-af93-9823247e0e33 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-284fdb0c-0f80-4ca7-af93-9823247e0e33');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-8224e2eb-dd8a-4496-900a-f4d2648a821c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8224e2eb-dd8a-4496-900a-f4d2648a821c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-8224e2eb-dd8a-4496-900a-f4d2648a821c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["pprint(df['Reviews'].values[0], width = 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6MTb_dgqTSS","executionInfo":{"status":"ok","timestamp":1705902483388,"user_tz":360,"elapsed":120,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"0003c803-8b40-45e7-9e07-db3d2acc042e"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["('New version of operation system is iOS 11. It is better than iOS 9.\\n'\n"," 'The new version of iPhone X seems cool.')\n"]}]},{"cell_type":"code","source":["pprint(df['Reviews'].values[1], width = 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HyucsMQ9q57B","executionInfo":{"status":"ok","timestamp":1705902493862,"user_tz":360,"elapsed":176,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"f2713bc1-b7a9-4c4b-85a6-cd0666686482"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["(' <p> The video of iphone x released. I liked iOS 9 but I like iOS 11 more.\\n'\n"," 'You may not like my like @Tech_Guru #Iphone #IOS harpreet@utdallas.edu  '\n"," 'https://jindal.utdallas.edu/')\n"]}]},{"cell_type":"code","source":["pprint(df['Reviews'].values[2], width = 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tRJgH7i4qOa6","executionInfo":{"status":"ok","timestamp":1705921556730,"user_tz":360,"elapsed":111,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"f083c2d7-ea17-429d-a6d9-b17bc87e1145"},"execution_count":160,"outputs":[{"output_type":"stream","name":"stdout","text":["('</p><p>The concept of regular expressions began in the 1950s, when the '\n"," 'American mathematician <a href=\"/wiki/Stephen_Cole_Kleene\" title=\"Stephen '\n"," 'Cole Kleene\">Stephen Cole Kleene</a> formalized the description of a <i><a '\n"," 'href=\"/wiki/Regular_language\" title=\"Regular language\">regular '\n"," 'language</a></i>. They came into common use with <a href=\"/wiki/Unix\" '\n"," 'title=\"Unix\">Unix</a> text-processing utilities. Different <a '\n"," 'href=\"/wiki/Syntax_(programming_languages)\" title=\"Syntax (programming '\n"," 'languages)\">syntaxes</a> for writing regular expressions have existed since '\n"," 'the 1980s, one being the <a href=\"/wiki/POSIX\" title=\"POSIX\">POSIX</a> '\n"," 'standard and another, widely used, being the <a href=\"/wiki/Perl\" '\n"," 'title=\"Perl\">Perl</a> syntax.\\n'\n"," '</p><p>Regular expressions are used in <a href=\"/wiki/Search_engine\" '\n"," 'title=\"Search engine\">search engines</a>, search and replace dialogs of <a '\n"," 'href=\"/wiki/Word_processor\" title=\"Word processor\">word processors</a> and '\n"," '<a href=\"/wiki/Text_editor\" title=\"Text editor\">text editors</a>, in <a '\n"," 'href=\"/wiki/Text_processing\" title=\"Text processing\">text processing</a> '\n"," 'utilities such as <a href=\"/wiki/Sed\" title=\"Sed\">sed</a> and <a '\n"," 'href=\"/wiki/AWK\" title=\"AWK\">AWK</a> and in <a href=\"/wiki/Lexical_analysis\" '\n"," 'title=\"Lexical analysis\">lexical analysis</a>. Many <a '\n"," 'href=\"/wiki/Programming_language\" title=\"Programming language\">programming '\n"," 'languages</a> provide regex capabilities either built-in or via <a '\n"," 'href=\"/wiki/Library_(computing)\" title=\"Library (computing)\">libraries</a>, '\n"," 'as it has uses in many situations.\\n'\n"," '</p> ')\n"]}]},{"cell_type":"code","source":["type(df['Reviews'].values[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b98LSsSMtt5p","executionInfo":{"status":"ok","timestamp":1705902538901,"user_tz":360,"elapsed":2,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"0df44f3e-c13f-4a96-f28f-c00a0d7564c8"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["str"]},"metadata":{},"execution_count":65}]},{"cell_type":"markdown","metadata":{"id":"o1yKR_gAnQPK"},"source":["# <font color = 'dodgerblue'>**Import Spacy Model**"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-01-30T09:05:01.617847Z","iopub.status.busy":"2023-01-30T09:05:01.617776Z","iopub.status.idle":"2023-01-30T09:05:05.855320Z","shell.execute_reply":"2023-01-30T09:05:05.854571Z","shell.execute_reply.started":"2023-01-30T09:05:01.617838Z"},"executionInfo":{"elapsed":14959,"status":"ok","timestamp":1705900789477,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"5oBaejG-xSLh","outputId":"c5b3d1fa-ccb3-40af-9998-3a8226ae48e7","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-01-22 05:19:36.086570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-22 05:19:36.086621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-22 05:19:36.087885: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-22 05:19:37.232673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n","Installing collected packages: en-core-web-sm\n","  Attempting uninstall: en-core-web-sm\n","    Found existing installation: en-core-web-sm 3.6.0\n","    Uninstalling en-core-web-sm-3.6.0:\n","      Successfully uninstalled en-core-web-sm-3.6.0\n","Successfully installed en-core-web-sm-3.7.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["# check the models we have dowloaded in spacy folder\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":173,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T09:05:05.856685Z","iopub.status.busy":"2023-01-30T09:05:05.856487Z","iopub.status.idle":"2023-01-30T09:05:06.232281Z","shell.execute_reply":"2023-01-30T09:05:06.231907Z","shell.execute_reply.started":"2023-01-30T09:05:05.856659Z"},"id":"kI4ZLLkcnQPT","tags":[],"executionInfo":{"status":"ok","timestamp":1705921724836,"user_tz":360,"elapsed":995,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# We will load the model -en_core_web_sm\n","nlp = spacy.load('en_core_web_sm')\n"]},{"cell_type":"markdown","metadata":{"id":"07hFKgezHnzW"},"source":["# <font color = 'dodgerblue'>**Pre-processing**"]},{"cell_type":"code","source":["def basic_clean(text: str) -> str:\n","    \"\"\"\n","    This function performs basic text cleaning on an input string by removing HTML tags (if present)\n","    and replacing newline and return characters with a space.\n","\n","    Parameters:\n","    text (str): The input string to be cleaned\n","\n","    Returns:\n","    str: The cleaned string\n","    \"\"\"\n","    # Use BeautifulSoup to remove HTML tags (if present)\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    text = soup.get_text()\n","\n","    # Replace newline and return characters with a space\n","    return re.sub(r'[\\n\\r]',' ', text)"],"metadata":{"id":"Px08vdQRoByY","executionInfo":{"status":"ok","timestamp":1705921726622,"user_tz":360,"elapsed":101,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":174,"outputs":[]},{"cell_type":"code","source":["cleaned_text= [basic_clean(text).encode('utf-8', 'ignore').decode() for text in df['Reviews'].values]"],"metadata":{"id":"okYIY1eYoD5W","executionInfo":{"status":"ok","timestamp":1705921729262,"user_tz":360,"elapsed":123,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":175,"outputs":[]},{"cell_type":"code","source":["pprint(cleaned_text[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ostbfo3SpTzw","executionInfo":{"status":"ok","timestamp":1705921730524,"user_tz":360,"elapsed":105,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"d9e321cb-335e-4b4b-f8b3-bd01b2a2afc2"},"execution_count":176,"outputs":[{"output_type":"stream","name":"stdout","text":["('The concept of regular expressions began in the 1950s, when the American '\n"," 'mathematician Stephen Cole Kleene formalized the description of a regular '\n"," 'language. They came into common use with Unix text-processing utilities. '\n"," 'Different syntaxes for writing regular expressions have existed since the '\n"," '1980s, one being the POSIX standard and another, widely used, being the Perl '\n"," 'syntax. Regular expressions are used in search engines, search and replace '\n"," 'dialogs of word processors and text editors, in text processing utilities '\n"," 'such as sed and AWK and in lexical analysis. Many programming languages '\n"," 'provide regex capabilities either built-in or via libraries, as it has uses '\n"," 'in many situations.  ')\n"]}]},{"cell_type":"code","execution_count":177,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T09:14:25.381942Z","iopub.status.busy":"2023-01-30T09:14:25.381624Z","iopub.status.idle":"2023-01-30T09:14:48.930788Z","shell.execute_reply":"2023-01-30T09:14:48.929670Z","shell.execute_reply.started":"2023-01-30T09:14:25.381927Z"},"id":"fIBqaxeJnQVt","tags":[],"executionInfo":{"status":"ok","timestamp":1705921732830,"user_tz":360,"elapsed":109,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["# initialize an empty list to store processed text\n","processed_text = []\n","\n","# initialize an empty list to store processed text\n","processed_text = []\n","\n","# Use context manager to temporarily disable the named pipes of spaCy NLP processing pipeline\n","with nlp.select_pipes(disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']):\n","  # process multiple documents in parallel using the spaCy NLP library\n","  for doc in nlp.pipe(cleaned_text, batch_size=3, n_process=1):\n","      # filter out URLs and emails, then extract text of each token\n","      tokens = [token.text for token in doc if not token.like_url and not token.like_email]\n","      text = ' '.join(tokens)\n","      # join tokens back into a single string and add to the processed_text list\n","      processed_text.append(text)"]},{"cell_type":"code","source":["cleaned_text[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"C4c9Jqneu19h","executionInfo":{"status":"ok","timestamp":1705921735563,"user_tz":360,"elapsed":108,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a209b5ae-2c7d-4aaa-ede5-4975ec0c1c18"},"execution_count":178,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'  The video of iphone x released. I liked iOS 9 but I like iOS 11 more. You may not like my like @Tech_Guru #Iphone #IOS harpreet@utdallas.edu  https://jindal.utdallas.edu/'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":178}]},{"cell_type":"code","execution_count":179,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"execution":{"iopub.execute_input":"2023-01-30T09:14:48.932492Z","iopub.status.busy":"2023-01-30T09:14:48.932344Z","iopub.status.idle":"2023-01-30T09:14:48.966025Z","shell.execute_reply":"2023-01-30T09:14:48.965594Z","shell.execute_reply.started":"2023-01-30T09:14:48.932477Z"},"executionInfo":{"elapsed":106,"status":"ok","timestamp":1705921746139,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"s1NHXq-iIOZ-","outputId":"950363bf-fa1c-4d98-b866-cb645d6b060f","tags":[]},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'   The video of iphone x released . I liked iOS 9 but I like iOS 11 more . You may not like my like @Tech_Guru # Iphone # IOS  '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":179}],"source":["processed_text[1]"]},{"cell_type":"code","source":["nlp = spacy.load('en_core_web_sm')\n","prefixes = list(nlp.Defaults.prefixes)\n","prefixes += ['@']\n","prefixes.remove(r'#')\n","\n","# Compile prefix regex based on selected prefixes\n","prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n","nlp.tokenizer.prefix_search = prefix_regex.search"],"metadata":{"id":"VARff10yuBCS","executionInfo":{"status":"ok","timestamp":1705921812945,"user_tz":360,"elapsed":853,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":183,"outputs":[]},{"cell_type":"code","source":["# initialize an empty list to store processed text\n","processed_text = []\n","\n","# Use context manager to temporarily disable the named pipes of spaCy NLP processing pipeline\n","with nlp.select_pipes(disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']):\n","  # process multiple documents in parallel using the spaCy NLP library\n","  for doc in nlp.pipe(cleaned_text, batch_size=3, n_process=1):\n","      # filter out URLs and emails, then extract text of each token\n","      tokens = [token.text for token in doc if not token.like_url and not token.like_email]\n","      text = ' '.join(tokens)\n","      # join tokens back into a single string and add to the processed_text list\n","      processed_text.append(text)"],"metadata":{"id":"qxgMd4KnwLOb","executionInfo":{"status":"ok","timestamp":1705921814865,"user_tz":360,"elapsed":106,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":184,"outputs":[]},{"cell_type":"code","source":["processed_text[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"65s4TpATwOKq","executionInfo":{"status":"ok","timestamp":1705921764240,"user_tz":360,"elapsed":330,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"dbc0d982-7684-4ce5-8b5d-ff236b05028a"},"execution_count":182,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'   The video of iphone x released . I liked iOS 9 but I like iOS 11 more . You may not like my like @ Tech_Guru #Iphone #IOS  '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":182}]},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from bs4 import BeautifulSoup\n","import re\n","import spacy\n","import numpy as np\n","from nltk.stem.porter import PorterStemmer\n","import os\n","\n","class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n","\n","    def __init__(self, model, *, batch_size = 64, lemmatize=True, lower=True, remove_stop=True,\n","                remove_punct=True, remove_email=True, remove_url=True, remove_num=False, stemming = False,\n","                add_user_mention_prefix=True, remove_hashtag_prefix=False, basic_clean_only=False):\n","\n","        self.model = model\n","        self.batch_size = batch_size\n","        self.remove_stop = remove_stop\n","        self.remove_punct = remove_punct\n","        self.remove_num = remove_num\n","        self.remove_url = remove_url\n","        self.remove_email = remove_email\n","        self.lower = lower\n","        self.add_user_mention_prefix = add_user_mention_prefix\n","        self.remove_hashtag_prefix = remove_hashtag_prefix\n","        self.basic_clean_only = basic_clean_only\n","\n","        if lemmatize and stemming:\n","            raise ValueError(\"Only one of 'lemmatize' and 'stemming' can be True.\")\n","\n","        self.lemmatize = lemmatize\n","        self.stemming = stemming\n","\n","    def basic_clean(self, text):\n","        soup = BeautifulSoup(text, \"html.parser\")\n","        text = soup.get_text()\n","        text = re.sub(r'[\\n\\r]', ' ', text)\n","        return text.strip()\n","\n","    def spacy_preprocessor(self, texts):\n","        final_result = []\n","        nlp = spacy.load(self.model)\n","\n","        # Disable unnecessary pipelines in spaCy model\n","        if self.lemmatize:\n","            # Disable parser and named entity recognition\n","            disabled_pipes = ['parser', 'ner']\n","        else:\n","            # Disable tagger, parser, attribute ruler, lemmatizer and named entity recognition\n","            disabled_pipes = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n","\n","        with nlp.select_pipes(disable=disabled_pipes):\n","          # Modify tokenizer behavior based on user_mention_prefix and hashtag_prefix settings\n","          if self.add_user_mention_prefix or self.remove_hashtag_prefix:\n","              prefixes = list(nlp.Defaults.prefixes)\n","              if self.add_user_mention_prefix:\n","                  prefixes += ['@']  # Treat '@' as a separate token\n","              if self.remove_hashtag_prefix:\n","                  prefixes.remove(r'#')  # Don't separate '#' from the following text\n","              prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n","              nlp.tokenizer.prefix_search = prefix_regex.search\n","\n","          # Process text data in parallel using spaCy's nlp.pipe()\n","          for doc in nlp.pipe(texts, batch_size=self.batch_size):\n","              filtered_tokens = []\n","              for token in doc:\n","                  # Check if token should be removed based on specified filters\n","                  if self.remove_stop and token.is_stop:\n","                      continue\n","                  if self.remove_punct and token.is_punct:\n","                      continue\n","                  if self.remove_num and token.like_num:\n","                      continue\n","                  if self.remove_url and token.like_url:\n","                      continue\n","                  if self.remove_email and token.like_email:\n","                      continue\n","\n","                  # Append the token's text, lemma, or stemmed form to the filtered_tokens list\n","                  if self.lemmatize:\n","                      filtered_tokens.append(token.lemma_)\n","                  elif self.stemming:\n","                      filtered_tokens.append(PorterStemmer().stem(token.text))\n","                  else:\n","                      filtered_tokens.append(token.text)\n","\n","              # Join the tokens and apply lowercasing if specified\n","              text = ' '.join(filtered_tokens)\n","              if self.lower:\n","                  text = text.lower()\n","              final_result.append(text.strip())\n","\n","        return final_result\n","\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X, y=None):\n","        try:\n","            if not isinstance(X, (list, np.ndarray)):\n","                raise TypeError(f'Expected list or numpy array, got {type(X)}')\n","\n","            x_clean = [self.basic_clean(text).encode('utf-8', 'ignore').decode() for text in X]\n","\n","            # Check if only basic cleaning is required\n","            if self.basic_clean_only:\n","                return x_clean  # Return the list of basic-cleaned texts\n","\n","            x_clean_final = self.spacy_preprocessor(x_clean)\n","            return x_clean_final\n","\n","        except Exception as error:\n","            print(f'An exception occurred: {repr(error)}')\n"],"metadata":{"id":"AOTSX3QKwQhp","executionInfo":{"status":"ok","timestamp":1705924909925,"user_tz":360,"elapsed":120,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":204,"outputs":[]},{"cell_type":"code","source":["# import spacy pre-processor from custom module\n","preprocessor = SpacyPreprocessor(model='en_core_web_sm', batch_size=64, lemmatize=False, lower=False,\n","                                    remove_stop=False, remove_punct=False, remove_email=False,\n","                                    remove_url=False, remove_num=False, stemming=False,\n","                                    add_user_mention_prefix=False, remove_hashtag_prefix=False, basic_clean_only=True)"],"metadata":{"id":"hXSpwrdFzUjs","executionInfo":{"status":"ok","timestamp":1705924946366,"user_tz":360,"elapsed":127,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":205,"outputs":[]},{"cell_type":"code","source":["cleaned_text = preprocessor.fit_transform(df['Reviews'].values)\n","\n","for item in cleaned_text:\n","    print()\n","    pprint(item, width = 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M2J9EjUUznTj","executionInfo":{"status":"ok","timestamp":1705927710578,"user_tz":360,"elapsed":187,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a86ae07f-918e-4498-ac43-f38f794f9cd4"},"execution_count":225,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","('New version of operation system is iOS 11. It is better than iOS 9. The new '\n"," 'version of iPhone X seems cool.')\n","\n","('The video of iphone x released. I liked iOS 9 but I like iOS 11 more. You '\n"," 'may not like my like @Tech_Guru #Iphone #IOS harpreet@utdallas.edu  '\n"," 'https://jindal.utdallas.edu/')\n","\n","('The concept of regular expressions began in the 1950s, when the American '\n"," 'mathematician Stephen Cole Kleene formalized the description of a regular '\n"," 'language. They came into common use with Unix text-processing utilities. '\n"," 'Different syntaxes for writing regular expressions have existed since the '\n"," '1980s, one being the POSIX standard and another, widely used, being the Perl '\n"," 'syntax. Regular expressions are used in search engines, search and replace '\n"," 'dialogs of word processors and text editors, in text processing utilities '\n"," 'such as sed and AWK and in lexical analysis. Many programming languages '\n"," 'provide regex capabilities either built-in or via libraries, as it has uses '\n"," 'in many situations.')\n"]}]},{"cell_type":"markdown","source":["# Feature Extraction"],"metadata":{"id":"HdzJxD936Zaq"}},{"cell_type":"code","source":["noun_count = [] # create a list to store the noun count for each document\n","aux_count = [] # create a list to store the auxiliary verb count for each document\n","verb_count = [] # create a list to store the verb count for each document\n","adj_count =[] # create a list to store the adjective count for each document\n","\n","# disable lemmatizer and named entity recognizer\n","disabled_pipes =  ['lemmatizer', 'ner']\n","\n","# iterate over the documents in the dataframe using the spacy pipe method\n","with nlp.select_pipes(disable=disabled_pipes):\n","  for doc in nlp.pipe(cleaned_text, batch_size=3, n_process=1):\n","\n","      # find all nouns and proper nouns in the document and store in a list\n","      nouns = [token.text for token in doc if (token.pos_ in [\"NOUN\",\"PROPN\"])]\n","\n","      # find all auxiliary verbs in the document and store in a list\n","      auxs =  [token.text for token in doc if (token.pos_ in [\"AUX\"])]\n","\n","      # find all verbs in the document and store in a list\n","      verbs =  [token.text for token in doc if (token.pos_ in [\"VERB\"])]\n","      print(verbs)\n","\n","      # find all adjectives in the document and store in a list\n","      adjectives =  [token.text for token in doc if (token.pos_ in [\"ADJ\"])]\n","\n","      # store the count of nouns in the noun_count list\n","      noun_count.append(len(nouns))\n","\n","      # store the count of auxiliary verbs in the aux_count list\n","      aux_count.append(len(auxs))\n","\n","      # store the count of verbs in the verb_count list\n","      verb_count.append(len(verbs))\n","\n","      # store the count of adjectives in the adj_count list\n","      adj_count.append(len(adjectives))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzYF4yIN3b57","executionInfo":{"status":"ok","timestamp":1705924979810,"user_tz":360,"elapsed":133,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"6bc861e6-4aba-49b5-cfde-71efd87b9a45"},"execution_count":207,"outputs":[{"output_type":"stream","name":"stdout","text":["['seems']\n","['released', 'liked', 'like', 'like']\n","['began', 'formalized', 'came', 'processing', 'writing', 'existed', 'used', 'replace', 'sed', 'provide', 'built']\n"]}]},{"cell_type":"code","source":["noun_count, verb_count, aux_count, adj_count"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XC-vq9Q6gGk","executionInfo":{"status":"ok","timestamp":1705924985027,"user_tz":360,"elapsed":116,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"42f1637d-fc85-47c0-ec85-c7a0aa49590d"},"execution_count":208,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([6, 8, 40], [1, 4, 11], [2, 1, 5], [4, 1, 12])"]},"metadata":{},"execution_count":208}]},{"cell_type":"code","source":["nlp = spacy.load('en_core_web_sm')\n","disabled_pipes = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n","with nlp.select_pipes(disable=disabled_pipes):\n","  if not nlp.has_pipe('sentencizer'):\n","    nlp.add_pipe('sentencizer')\n","\n","    list_count_words = []\n","    list_count_characters = []\n","    list_count_characters_no_space = []\n","    list_avg_word_length = []\n","    list_count_digits = []\n","    list_count_numbers = []\n","    list_count_sentences = []\n","\n","    for doc in nlp.pipe(cleaned_text, batch_size=3, n_process=1):\n","        # Count words (tokens)\n","        count_word = len([token for token in doc if not token.is_punct])\n","\n","        # Count all characters (including spaces)\n","        count_char = len(doc.text)\n","\n","        # Count characters without spaces\n","        count_char_no_space = len(doc.text_with_ws.replace(' ', ''))\n","\n","        # Calculate average word length\n","        avg_word_length = count_char_no_space / (count_word + 1)\n","\n","        # Count numbers (consecutive digits)\n","        count_numbers = len([token for token in doc if token.is_digit])\n","\n","        # Count sentences\n","        count_sentences = len(list(doc.sents))\n","\n","        list_count_words.append(count_word)\n","        list_count_characters.append(count_char)\n","        list_count_characters_no_space.append(count_char_no_space)\n","        list_avg_word_length.append(avg_word_length)\n","        list_count_numbers.append(count_numbers)\n","        list_count_sentences.append(count_sentences)"],"metadata":{"id":"uEF4f8Qq5gKM","executionInfo":{"status":"ok","timestamp":1705925080057,"user_tz":360,"elapsed":733,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":211,"outputs":[]},{"cell_type":"code","source":["list_count_words,  list_count_characters, list_count_characters_no_space, list_avg_word_length, list_count_numbers, list_count_sentences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zzem6gow7gG9","executionInfo":{"status":"ok","timestamp":1705925083064,"user_tz":360,"elapsed":133,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"484182b6-6287-47d1-dd91-48e12588d4b6"},"execution_count":212,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([22, 28, 106],\n"," [107, 170, 687],\n"," [86, 143, 584],\n"," [3.739130434782609, 4.931034482758621, 5.457943925233645],\n"," [2, 2, 0],\n"," [3, 3, 5])"]},"metadata":{},"execution_count":212}]},{"cell_type":"code","source":["for item in cleaned_text:\n","    print()\n","    pprint(item, width = 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGg_v0jG7tC8","executionInfo":{"status":"ok","timestamp":1705925143948,"user_tz":360,"elapsed":127,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"0082ff02-60be-400a-f414-9d9aadc45f3e"},"execution_count":214,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","('New version of operation system is iOS 11. It is better than iOS 9. The new '\n"," 'version of iPhone X seems cool.')\n","\n","('The video of iphone x released. I liked iOS 9 but I like iOS 11 more. You '\n"," 'may not like my like @Tech_Guru #Iphone #IOS harpreet@utdallas.edu  '\n"," 'https://jindal.utdallas.edu/')\n","\n","('The concept of regular expressions began in the 1950s, when the American '\n"," 'mathematician Stephen Cole Kleene formalized the description of a regular '\n"," 'language. They came into common use with Unix text-processing utilities. '\n"," 'Different syntaxes for writing regular expressions have existed since the '\n"," '1980s, one being the POSIX standard and another, widely used, being the Perl '\n"," 'syntax. Regular expressions are used in search engines, search and replace '\n"," 'dialogs of word processors and text editors, in text processing utilities '\n"," 'such as sed and AWK and in lexical analysis. Many programming languages '\n"," 'provide regex capabilities either built-in or via libraries, as it has uses '\n"," 'in many situations.')\n"]}]},{"cell_type":"code","source":["nlp = spacy.load('en_core_web_sm')\n","count_ner = []\n","disabled_pipes = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n","with nlp.select_pipes(disable=disabled_pipes):\n","# Disable the tok2vec, tagger, parser, attribute ruler, and lemmatizer pipelines for improved performance\n","  for doc in nlp.pipe(cleaned_text, batch_size=1000, n_process=-1):\n","      ner_text = [ent.text for ent in doc.ents]\n","      print(ner_text)\n","      ner_labels = [ent.label_ for ent in doc.ents]\n","      print(ner_labels)\n","      count_ner.append(len(ner_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YN7pr8Jo7ysE","executionInfo":{"status":"ok","timestamp":1705927872512,"user_tz":360,"elapsed":1017,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"cb1267ba-68bf-4097-e890-5ec8dced198d"},"execution_count":229,"outputs":[{"output_type":"stream","name":"stdout","text":["['11', '9']\n","['CARDINAL', 'CARDINAL']\n","['9', '11', 'IOS']\n","['CARDINAL', 'CARDINAL', 'ORG']\n","['the 1950s', 'American', 'Stephen Cole', 'the 1980s', 'one', 'POSIX', 'Perl', 'AWK']\n","['DATE', 'NORP', 'PERSON', 'DATE', 'CARDINAL', 'ORG', 'ORG', 'ORG']\n"]}]},{"cell_type":"code","source":["count_ner"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JhLpEe8bFtA2","executionInfo":{"status":"ok","timestamp":1705927874013,"user_tz":360,"elapsed":250,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"c1488860-d669-4059-b7a9-8059af4180a9"},"execution_count":230,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 3, 8]"]},"metadata":{},"execution_count":230}]},{"cell_type":"code","source":["from sklearn.base import TransformerMixin, BaseEstimator\n","import numpy as np\n","import spacy\n","import re\n","import sys\n","import os\n","from pathlib import Path\n","\n","\n","class ManualFeatures(TransformerMixin, BaseEstimator):\n","\n","    \"\"\"A transformer class for extracting manual features from text data.\n","\n","    This class is designed to be used in a scikit-learn pipeline. It uses the spaCy\n","    library to extract a variety of manual features from text data, such as\n","    part-of-speech (POS) features, named entity recognition (NER) features,\n","    and count-based features.\n","    \"\"\"\n","\n","\n","\n","    def __init__(self, spacy_model, batch_size = 64, pos_features = True, ner_features = True, count_features = True):\n","\n","        \"\"\"\n","        Initialize the feature extractor.\n","\n","        Parameters\n","        ----------\n","        spacy_model : str\n","            The name of the spaCy model to use for feature extraction.\n","        pos_features : bool, optional (default=True)\n","            Whether to extract part-of-speech (POS) features from the text data.\n","        ner_features : bool, optional (default=True)\n","            Whether to extract named entity recognition (NER) features from the text data.\n","        count_features : bool, optional (default=True)\n","            Whether to extract count-based features from the text data.\n","        \"\"\"\n","\n","        self.spacy_model = spacy_model\n","        self.batch_size = batch_size\n","        self.pos_features = pos_features\n","        self.ner_features = ner_features\n","        self.count_features = count_features\n","\n","    def get_cores(self):\n","        \"\"\"\n","        Get the number of CPU cores to use in parallel processing.\n","        \"\"\"\n","        # Get the number of CPU cores available on the system.\n","        num_cores = os.cpu_count()\n","        if num_cores < 3:\n","            use_cores = 1\n","        else:\n","            use_cores = num_cores // 2 + 1\n","        return num_cores\n","\n","    def get_pos_features(self, cleaned_text):\n","\n","        nlp = spacy.load(self.spacy_model)\n","        noun_count = []\n","        aux_count = []\n","        verb_count = []\n","        adj_count =[]\n","\n","        # Disable the lemmatizer and NER pipelines for improved performance\n","        disabled_pipes = ['lemmatizer', 'ner']\n","        with nlp.select_pipes(disable=disabled_pipes):\n","            n_process = self.get_cores()\n","            for doc in nlp.pipe(cleaned_text, batch_size=self.batch_size, n_process=n_process):\n","                # Extract nouns, auxiliaries, verbs, and adjectives from the document\n","                nouns = [token.text for token in doc if token.pos_ in [\"NOUN\",\"PROPN\"]]\n","                auxs =  [token.text for token in doc if token.pos_ in [\"AUX\"]]\n","                verbs =  [token.text for token in doc if token.pos_ in [\"VERB\"]]\n","                adjectives =  [token.text for token in doc if token.pos_ in [\"ADJ\"]]\n","\n","                # Store the count of each type of word in separate lists\n","                noun_count.append(len(nouns))\n","                aux_count.append(len(auxs))\n","                verb_count.append(len(verbs))\n","                adj_count.append(len(adjectives))\n","\n","        # Stack the count lists vertically to form a 2D numpy array\n","        return np.transpose(np.vstack((noun_count, aux_count, verb_count, adj_count)))\n","\n","\n","\n","    def get_ner_features(self, cleaned_text):\n","        nlp = spacy.load(self.spacy_model)\n","        count_ner = []\n","\n","        # Disable the tok2vec, tagger, parser, attribute ruler, and lemmatizer pipelines for improved performance\n","        disabled_pipes = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n","        with nlp.select_pipes(disable=disabled_pipes):\n","            n_process = self.get_cores()\n","            for doc in nlp.pipe(cleaned_text, batch_size=self.batch_size, n_process=n_process):\n","                ners = [ent.label_ for ent in doc.ents]\n","                count_ner.append(len(ners))\n","\n","        # Convert the list of NER counts to a 2D numpy array\n","        return np.array(count_ner).reshape(-1, 1)\n","\n","\n","    def get_count_features(self, cleaned_text):\n","        list_count_words = []\n","        list_count_characters = []\n","        list_count_characters_no_space = []\n","        list_avg_word_length = []\n","        list_count_digits = []\n","        list_count_numbers = []\n","        list_count_sentences = []\n","\n","        nlp = spacy.load(self.spacy_model)\n","        disabled_pipes = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n","        with nlp.select_pipes(disable=disabled_pipes):\n","            if not nlp.has_pipe('sentencizer'):\n","                nlp.add_pipe('sentencizer')\n","            n_process = self.get_cores()\n","            for doc in nlp.pipe(cleaned_text, batch_size=self.batch_size, n_process=n_process):\n","                count_word = len([token for token in doc if not token.is_punct])\n","                count_char = len(doc.text)\n","                count_char_no_space = len(doc.text_with_ws.replace(' ', ''))\n","                avg_word_length = count_char_no_space / (count_word + 1)\n","                count_numbers = len([token for token in doc if token.is_digit])\n","                count_sentences = len(list(doc.sents))\n","\n","                list_count_words.append(count_word)\n","                list_count_characters.append(count_char)\n","                list_count_characters_no_space.append(count_char_no_space)\n","                list_avg_word_length.append(avg_word_length)\n","                list_count_numbers.append(count_numbers)\n","                list_count_sentences.append(count_sentences)\n","\n","        count_features = np.vstack((list_count_words, list_count_characters, list_count_characters_no_space, list_avg_word_length,\n","                                    list_count_numbers, list_count_sentences))\n","        return np.transpose(count_features)\n","\n","\n","    def fit(self, X, y=None):\n","        \"\"\"\n","        Fit the feature extractor to the input data.\n","\n","        This method does not actually do any fitting, as the feature extractor is stateless.\n","        It simply returns the instance of the class.\n","\n","        Parameters:\n","        X (list or numpy.ndarray): The input data.\n","        y (list or numpy.ndarray, optional): The target labels. Not used in this implementation.\n","\n","        Returns:\n","        FeatureExtractor: The instance of the class.\n","        \"\"\"\n","        return self\n","\n","\n","    def transform(self, X, y=None):\n","        \"\"\"\n","        Transform the input data into a set of features.\n","\n","        Parameters:\n","        X (list or numpy.ndarray): The input data.\n","        y (list or numpy.ndarray, optional): The target labels. Not used in this implementation.\n","\n","        Returns:\n","        tuple: A tuple containing a 2D numpy array with shape (len(X), num_features) where num_features is the number of features extracted and a list of feature names.\n","\n","        Raises:\n","        TypeError: If the input data is not a list or numpy array.\n","        Exception: If an error occurs while transforming the data into features.\n","        \"\"\"\n","        try:\n","            # Check if the input data is a list or numpy array\n","            if not isinstance(X, (list, np.ndarray)):\n","                raise TypeError(f\"Expected list or numpy array, got {type(X)}\")\n","\n","\n","            feature_names = []\n","\n","            if self.count_features:\n","                count_features = self.get_count_features(X)\n","                feature_names.extend(['count_words', 'count_characters',\n","                                      'count_characters_no_space', 'avg_word_length',\n","                                      'count_numbers', 'count_sentences'])\n","            else:\n","                count_features = np.empty(shape=(0, 0))\n","\n","            if self.pos_features:\n","                pos_features = self.get_pos_features(X)\n","                feature_names.extend(['noun_count', 'aux_count', 'verb_count', 'adj_count'])\n","            else:\n","                pos_features = np.empty(shape=(0, 0))\n","\n","            if self.ner_features:\n","                ner_features = self.get_ner_features(X)\n","                feature_names.extend(['ner'])\n","            else:\n","                ner_features = np.empty(shape=(0, 0))\n","\n","            # Stack the feature arrays horizontally to form a single 2D numpy array\n","            return np.hstack((count_features, pos_features,  ner_features)), feature_names\n","\n","        except Exception as error:\n","            print(f'An exception occured: {repr(error)}')"],"metadata":{"id":"L3QfePsnJsBN","executionInfo":{"status":"ok","timestamp":1705928133110,"user_tz":360,"elapsed":154,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":235,"outputs":[]},{"cell_type":"code","source":["featurizer = ManualFeatures(spacy_model='en_core_web_sm', batch_size =3)"],"metadata":{"id":"As7hbiB_NACF","executionInfo":{"status":"ok","timestamp":1705928136460,"user_tz":360,"elapsed":128,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":236,"outputs":[]},{"cell_type":"code","source":["X_train_features, feature_names = featurizer.fit_transform(cleaned_text )"],"metadata":{"id":"18oNzjR5Nie2","executionInfo":{"status":"ok","timestamp":1705928140648,"user_tz":360,"elapsed":2737,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"execution_count":237,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(X_train_features, columns=feature_names )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"yztZv1coN4lI","executionInfo":{"status":"ok","timestamp":1705928142650,"user_tz":360,"elapsed":136,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"a9de3e65-051b-4d0a-bc3f-d5ceab4158e5"},"execution_count":238,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   count_words  count_characters  count_characters_no_space  avg_word_length  \\\n","0         22.0             107.0                       86.0         3.739130   \n","1         28.0             170.0                      143.0         4.931034   \n","2        106.0             687.0                      584.0         5.457944   \n","\n","   count_numbers  count_sentences  noun_count  aux_count  verb_count  \\\n","0            2.0              3.0         6.0        2.0         1.0   \n","1            2.0              3.0         8.0        1.0         4.0   \n","2            0.0              5.0        40.0        5.0        11.0   \n","\n","   adj_count  ner  \n","0        4.0  2.0  \n","1        1.0  3.0  \n","2       12.0  8.0  "],"text/html":["\n","  <div id=\"df-174b289b-18eb-40ff-abd8-7143990b65bd\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count_words</th>\n","      <th>count_characters</th>\n","      <th>count_characters_no_space</th>\n","      <th>avg_word_length</th>\n","      <th>count_numbers</th>\n","      <th>count_sentences</th>\n","      <th>noun_count</th>\n","      <th>aux_count</th>\n","      <th>verb_count</th>\n","      <th>adj_count</th>\n","      <th>ner</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>22.0</td>\n","      <td>107.0</td>\n","      <td>86.0</td>\n","      <td>3.739130</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>6.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>28.0</td>\n","      <td>170.0</td>\n","      <td>143.0</td>\n","      <td>4.931034</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>8.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>106.0</td>\n","      <td>687.0</td>\n","      <td>584.0</td>\n","      <td>5.457944</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>40.0</td>\n","      <td>5.0</td>\n","      <td>11.0</td>\n","      <td>12.0</td>\n","      <td>8.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-174b289b-18eb-40ff-abd8-7143990b65bd')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-174b289b-18eb-40ff-abd8-7143990b65bd button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-174b289b-18eb-40ff-abd8-7143990b65bd');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-83d3fe91-a2b2-4800-96ba-d9d99e12e65d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-83d3fe91-a2b2-4800-96ba-d9d99e12e65d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-83d3fe91-a2b2-4800-96ba-d9d99e12e65d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":238}]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}